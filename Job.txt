WARN $TMPDIR(/tmp/SLURM_21302623_20158/glide_tHsSit/execute/dir_8263) is not a writable directory setting $TMPDIR = $PWD
======== gWMS-CMSRunAnalysis.sh STARTING at Tue May  8 10:18:48 GMT 2018 on c0121.brazos.tamu.edu ========
Local time : Tue May  8 10:18:48 UTC 2018
Current system : Linux c0121.brazos.tamu.edu 4.4.128-1.el6.elrepo.x86_64 #1 SMP Sat Apr 14 08:41:22 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
Arguments are -a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=1 --cmsswVersion=CMSSW_9_4_1 --scramArch=slc6_amd64_gcc630 --inputFile=job_input_file_list_1.txt --runAndLumis=job_lumis_1.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 --scriptArgs=[] -o {}
SCRAM_ARCH=slc6_amd64_gcc630
======== HTCONDOR JOB SUMMARY at Tue May  8 10:18:48 GMT 2018 START ========
CRAB ID: 1
Execution site: T3_US_TAMU
Current hostname: c0121.brazos.tamu.edu
Output files: out_raw.root=out_raw_1.root
==== HTCONDOR JOB AD CONTENTS START ====
== JOB AD: ProvisionedResources = "Cpus Memory Disk Swap"
== JOB AD: CRAB_PrimaryDataset = "DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8"
== JOB AD: CumulativeRemoteUserCpu = 0.0
== JOB AD: RequestMemory = 2000
== JOB AD: CMS_ALLOW_OVERFLOW = "True"
== JOB AD: TransferOutput = "jobReport.json.1,WMArchiveReport.json.1"
== JOB AD: JobStatus = 2
== JOB AD: CRAB_TaskEndTime = 1528364119
== JOB AD: CoreSize = -1
== JOB AD: JOB_GLIDEIN_SiteWMS = "SLURM"
== JOB AD: DESIRED_Archs = "X86_64"
== JOB AD: JOB_GLIDECLIENT_Name = "CMSG-v1_0.local_users"
== JOB AD: JOB_GLIDEIN_Entry_Name = "CMS_T3_TAMU_BRAZOS_ce01"
== JOB AD: Used_Gatekeeper = "ce01.brazos.tamu.edu ce01.brazos.tamu.edu:9619"
== JOB AD: EncryptExecuteDirectory = false
== JOB AD: JOB_Site = "TAMU_BRAZOS"
== JOB AD: StartdPrincipal = "execute-side@matchsession/165.91.55.24"
== JOB AD: CRAB_SiteBlacklist = {  }
== JOB AD: JOB_GLIDEIN_Schedd = "schedd_glideins5@vocms0805.cern.ch"
== JOB AD: TargetType = "Machine"
== JOB AD: NiceUser = false
== JOB AD: ShadowBday = 1525774721
== JOB AD: DESIRED_Overflow_Region = strcat(ifthenelse(OVERFLOW_US =?= "True","US","none"),",",ifthenelse(OVERFLOW_IT =?= "True","IT","none"),",",ifthenelse(OVERFLOW_UK =?= "True","UK","none"))
== JOB AD: TransferIn = false
== JOB AD: NumCkpts_RAW = 0
== JOB AD: ExecutableSize = 10
== JOB AD: JobRunCount = 1
== JOB AD: CRAB_oneEventMode = 0
== JOB AD: JOB_GLIDEIN_MaxMemMBs = "2500"
== JOB AD: CRAB_RestURInoAPI = "/crabserver/prod"
== JOB AD: JOB_GLIDEIN_ClusterId = "1255353"
== JOB AD: CommittedSlotTime = 0
== JOB AD: PeriodicRelease = ( HoldReasonCode == 28 ) || ( HoldReasonCode == 30 ) || ( HoldReasonCode == 13 ) || ( HoldReasonCode == 6 )
== JOB AD: User = "cms280@cms"
== JOB AD: CRAB_RestHost = "cmsweb.cern.ch"
== JOB AD: DiskProvisioned = 77597232
== JOB AD: ExecutableSize_RAW = 9
== JOB AD: CRAB_OutLFNDir = "/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/DIGI_RAW/180508_093519"
== JOB AD: x509UserProxyFirstFQAN = "/cms/Role=NULL/Capability=NULL"
== JOB AD: NumRestarts = 0
== JOB AD: JOB_GLIDEIN_Site = "TAMU_BRAZOS"
== JOB AD: TotalSubmitProcs = 1
== JOB AD: LastJobLeaseRenewal = 1525774722
== JOB AD: SubmitEventNotes = "DAG Node: Job1"
== JOB AD: DAGParentNodeNames = ""
== JOB AD: AutoClusterId = 71230
== JOB AD: CRAB_RetryOnASOFailures = 1
== JOB AD: HasBeenTimingTuned = true
== JOB AD: JobLeaseDuration = 2400
== JOB AD: CRAB_ASODB = "filetransfers"
== JOB AD: Requirements = ( ( ( target.IS_GLIDEIN =!= true ) || ( target.GLIDEIN_CMSSite =!= undefined ) ) ) && ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
== JOB AD: ShadowIpAddr = "<188.184.89.21:4080?addrs=188.184.89.21-4080&noUDP&sock=3440400_c878_713237>"
== JOB AD: LocalSysCpu = 0.0
== JOB AD: x509UserProxyEmail = "ws13@rice.edu"
== JOB AD: Arguments = "-a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=1 --cmsswVersion=CMSSW_9_4_1 --scramArch=slc6_amd64_gcc630 --inputFile=job_input_file_list_1.txt --runAndLumis=job_lumis_1.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 --scriptArgs=[] -o {}"
== JOB AD: PeriodicRemove = ( ( JobStatus =?= 5 ) && ( time() - EnteredCurrentStatus > 7 * 60 ) ) || ( ( JobStatus =?= 1 ) && ( time() - EnteredCurrentStatus > 7 * 24 * 60 * 60 ) ) || ( ( JobStatus =?= 2 ) && ( ( MemoryUsage > RequestMemory ) || ( MaxWallTimeMins * 60 < time() - EnteredCurrentStatus ) || ( DiskUsage > 20000000 ) ) ) || ( time() > CRAB_TaskEndTime ) || ( ( JobStatus =?= 1 ) && ( time() > ( x509UserProxyExpiration + 86400 ) ) )
== JOB AD: MachineAttrTotalSlotCpus0 = 1
== JOB AD: JobStartDate = 1525774721
== JOB AD: RemoteHost = "glidein_29322_251210436@c0121.brazos.tamu.edu"
== JOB AD: OrigIwd = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0"
== JOB AD: CRAB_UserGroup = undefined
== JOB AD: PostJobPrio1 = -1525772185
== JOB AD: EstimatedWallTimeMins = 60
== JOB AD: Err = "_condor_stderr"
== JOB AD: PostJobPrio2 = 2
== JOB AD: ShadowVersion = "$CondorVersion: 8.6.8 Oct 30 2017 BuildID: 422919 $"
== JOB AD: NumSystemHolds = 0
== JOB AD: GlobalJobId = "crab3@vocms0120.cern.ch#20916852.0#1525774428"
== JOB AD: JOBGLIDEIN_CMSSite = "T3_US_TAMU"
== JOB AD: PublicClaimId = "<192.168.200.30:45035>#1525774023#3#..."
== JOB AD: Environment = "CRAB_TASKMANAGER_TARBALL=local SCRAM_ARCH=slc6_amd64_gcc630 CRAB_RUNTIME_TARBALL=local"
== JOB AD: CRAB_UserDN = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi"
== JOB AD: RequestMemory_RAW = 2000
== JOB AD: PeriodicHold = false
== JOB AD: ProcId = 0
== JOB AD: CRAB_PublishName = "DIGI_RAW-00000000000000000000000000000000"
== JOB AD: StartdIpAddr = "<192.168.200.30:45035?CCBID=131.225.205.232:9664%3faddrs%3d131.225.205.232-9664#4820227%20188.184.83.197:9664%3faddrs%3d188.184.83.197-9664#5958373&addrs=192.168.200.30-45035+[--1]-45035&noUDP>"
== JOB AD: x509UserProxyVOName = "cms"
== JOB AD: OnExitHold = false
== JOB AD: x509userproxysubject = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi"
== JOB AD: CRAB_DataBlock = "/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/wshi-GEN_SIM-21fd64682e845e630b293e515db358b1/USER#8b0ce5c3-1766-4903-8c5d-2c7fc04a403a"
== JOB AD: TotalSuspensions = 0
== JOB AD: LeaveJobInQueue = false
== JOB AD: REQUIRED_OS = "rhel6"
== JOB AD: CMSGroups = "/cms,T3_US_TAMU"
== JOB AD: CRAB_ISB = "https://cmsweb.cern.ch/crabcache"
== JOB AD: OrigMaxHosts = 1
== JOB AD: CRAB_AsyncDest = "T3_US_TAMU"
== JOB AD: NumCkpts = 0
== JOB AD: DAGNodeName = "Job1"
== JOB AD: Out = "_condor_stdout"
== JOB AD: NumJobCompletions = 0
== JOB AD: CRAB_UserHN = "wshi"
== JOB AD: AcctGroupUser = "wshi"
== JOB AD: JobPrio = 10
== JOB AD: CRAB_TaskLifetimeDays = 30
== JOB AD: CRAB_PublishGroupName = 0
== JOB AD: WantRemoteIO = true
== JOB AD: RootDir = "/"
== JOB AD: DESIRED_CMSDataset = "/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/wshi-GEN_SIM-21fd64682e845e630b293e515db358b1/USER"
== JOB AD: WantCheckpoint = false
== JOB AD: LastTimingTuned = 1525774665
== JOB AD: OrigCmd = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0/gWMS-CMSRunAnalysis.sh"
== JOB AD: JOB_GLIDEIN_Factory = "CMS-CERN-Production"
== JOB AD: MachineAttrMJF_JOB_HS06_JOB0 = "Unknown"
== JOB AD: CpusProvisioned = 1
== JOB AD: RequestDisk_RAW = 1
== JOB AD: JOB_GLIDEIN_Memory = "2500"
== JOB AD: WhenToTransferOutput = "ON_EXIT_OR_EVICT"
== JOB AD: CRAB_AdditionalOutputFiles = {  }
== JOB AD: ExitStatus = 0
== JOB AD: MachineAttrDIRACBenchmark0 = 8.12743823153
== JOB AD: CurrentHosts = 1
== JOB AD: BufferSize = 524288
== JOB AD: CumulativeRemoteSysCpu = 0.0
== JOB AD: CRAB_PublishDBSURL = "https://cmsweb.cern.ch/dbs/prod/phys03/DBSWriter"
== JOB AD: NumJobStarts = 0
== JOB AD: CRAB_OutTempLFNDir = "/store/temp/user/wshi.63a69f219e26fe6d24cb66b819d1010afce47764/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/DIGI_RAW/180508_093519"
== JOB AD: MaxWallTimeMins_RAW = 1250
== JOB AD: LastSuspensionTime = 0
== JOB AD: MaxHosts = 1
== JOB AD: CRAB_NumAutomJobRetries = 2
== JOB AD: OVERFLOW_IT = ifthenelse(regexp("T[1,2]_IT_",DESIRED_Sites),"True",undefined)
== JOB AD: MinHosts = 1
== JOB AD: JOB_GLIDEIN_SiteWMS_Slot = "Unknown"
== JOB AD: CRAB_JobSW = "CMSSW_9_4_1"
== JOB AD: UidDomain = "cms"
== JOB AD: Owner = "cms280"
== JOB AD: DelegatedProxyExpiration = 1525861122
== JOB AD: ShouldTransferFiles = "YES"
== JOB AD: CRAB_JobType = "analysis"
== JOB AD: CRAB_SaveLogsFlag = 0
== JOB AD: ExitBySignal = false
== JOB AD: JobAdInformationAttrs = "MATCH_EXP_JOBGLIDEIN_CMSSite, JOBGLIDEIN_CMSSite, RemoteSysCpu, RemoteUserCpu"
== JOB AD: WantRemoteSyscalls = false
== JOB AD: CRAB_ASOTimeout = 86400
== JOB AD: CompletionDate = 0
== JOB AD: TransferInput = "CMSRunAnalysis.sh,cmscp.py,CMSRunAnalysis.tar.gz,sandbox.tar.gz,run_and_lumis.tar.gz,input_files.tar.gz"
== JOB AD: CumulativeSuspensionTime = 0
== JOB AD: JOB_GLIDEIN_ToDie = "1526011817"
== JOB AD: JOB_GLIDEIN_ProcId = "7"
== JOB AD: In = "/dev/null"
== JOB AD: RemoteSlotID = 1
== JOB AD: CRAB_UserRole = undefined
== JOB AD: MyType = "Job"
== JOB AD: DiskUsage_RAW = 4235
== JOB AD: JOB_GLIDEIN_Name = "v3_2"
== JOB AD: CRAB_Publish = 1
== JOB AD: JOB_GLIDEIN_Max_Walltime = "432000"
== JOB AD: CRAB_Workflow = "180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100"
== JOB AD: MaxWallTimeMins = 1250
== JOB AD: JOB_GLIDEIN_SEs = "srm.brazos.tamu.edu"
== JOB AD: CRAB_EDMOutputFiles = { "out_raw.root" }
== JOB AD: BufferBlockSize = 32768
== JOB AD: MemoryProvisioned = 2500
== JOB AD: TransferInputSizeMB = 4
== JOB AD: CRAB_DBSURL = "https://cmsweb.cern.ch/dbs/prod/phys03/DBSReader"
== JOB AD: TransferSocket = "<188.184.89.21:4080?addrs=188.184.89.21-4080&noUDP&sock=3440400_c878_713237>"
== JOB AD: JOB_GLIDEIN_SiteWMS_Queue = "Unknown"
== JOB AD: StreamErr = false
== JOB AD: MyAddress = "<188.184.89.21:4080?addrs=188.184.89.21-4080&noUDP&sock=3440400_c878_713237>"
== JOB AD: PeriodicRemoveReason = ifThenElse(time() - EnteredCurrentStatus > 7 * 24 * 60 * 60 && isUndefined(MemoryUsage),"Removed due to idle time limit",ifThenElse(time() > x509UserProxyExpiration,"Removed job due to proxy expiration",ifThenElse(MemoryUsage > RequestMemory,"Removed due to memory use",ifThenElse(MaxWallTimeMins * 60 < time() - EnteredCurrentStatus,"Removed due to wall clock limit",ifThenElse(DiskUsage > 20000000,"Removed due to disk usage",ifThenElse(time() > CRAB_TaskEndTime,"Removed due to reached CRAB_TaskEndTime","Removed due to job being held"))))))
== JOB AD: OVERFLOW_CHECK = ifthenelse(MATCH_GLIDEIN_CMSSite =!= undefined,ifthenelse(stringListMember(MATCH_GLIDEIN_CMSSite,DESIRED_Sites),false,true),false)
== JOB AD: CommittedTime = 0
== JOB AD: RequestDisk = 100000
== JOB AD: AcctGroup = "analysis"
== JOB AD: LocalUserCpu = 0.0
== JOB AD: CRAB_localOutputFiles = "out_raw.root=out_raw_1.root"
== JOB AD: NumJobMatches = 1
== JOB AD: CRAB_JobArch = "slc6_amd64_gcc630"
== JOB AD: MachineAttrCpus0 = 1
== JOB AD: DAGManJobId = 20914060
== JOB AD: DAGManNodesMask = "0,1,2,4,5,7,9,10,11,12,13,16,17,24,27"
== JOB AD: HasBeenRouted = false
== JOB AD: MachineAttrSlotWeight0 = 1
== JOB AD: RemoteUserCpu = 0.0
== JOB AD: LastJobStatus = 1
== JOB AD: UserLog = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0/job_log"
== JOB AD: ImageSize = 10
== JOB AD: JOB_CMSSite = "T3_US_TAMU"
== JOB AD: DESIRED_SITES = "T3_US_TAMU"
== JOB AD: OVERFLOW_UK = ifthenelse(regexp("T2_UK_London_",DESIRED_Sites),"True",undefined)
== JOB AD: TaskType = "Job"
== JOB AD: Iwd = "/tmp/SLURM_21302623_20158/glide_tHsSit/execute/dir_8263"
== JOB AD: CRAB_ASOURL = "https://cmsweb.cern.ch/crabserver/prod"
== JOB AD: ImageSize_RAW = 9
== JOB AD: DAGManNodesLog = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0/RunJobs.dag.nodes.log"
== JOB AD: StreamOut = false
== JOB AD: JobUniverse = 5
== JOB AD: OVERFLOW_US = ifthenelse(regexp("T[1,2]_US_",DESIRED_Sites),"True",undefined)
== JOB AD: QDate = 1525774428
== JOB AD: SpoolOnEvict = false
== JOB AD: EnteredCurrentStatus = 1525774718
== JOB AD: CRAB_ReqName = "180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100"
== JOB AD: DESIRED_CMSDataLocations = "T3_US_TAMU"
== JOB AD: CRAB_SiteWhitelist = { "T3_US_TAMU" }
== JOB AD: x509UserProxyExpiration = 1526376960
== JOB AD: NumShadowStarts = 1
== JOB AD: CommittedSuspensionTime = 0
== JOB AD: LastMatchTime = 1525774721
== JOB AD: PreJobPrio1 = 0
== JOB AD: JOB_GLIDEIN_SiteWMS_JobId = "21302623"
== JOB AD: JobNotification = 0
== JOB AD: x509userproxy = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0/aae00b45be2ec79d71339d457d5a9788729ac18a"
== JOB AD: CRAB_TFileOutputFiles = {  }
== JOB AD: JobBatchName = "RunJobs.dag+20914060"
== JOB AD: CumulativeSlotTime = 0
== JOB AD: CRAB_TransferOutputs = 1
== JOB AD: RemoteSysCpu = 0.0
== JOB AD: CRAB_SubmitterIpAddr = "188.185.67.179"
== JOB AD: JOB_GLIDEIN_CMSSite = "T3_US_TAMU"
== JOB AD: CondorPlatform = "$CondorPlatform: x86_64_RedHat6 $"
== JOB AD: OnExitRemove = true
== JOB AD: Rank = 0.0
== JOB AD: AutoClusterAttrs = "JobUniverse,LastCheckpointPlatform,NumCkpts,NONE,_condor_RequestCpus,_condor_RequestDisk,_condor_RequestIoslots,_condor_RequestMemory,_condor_RequestRepackslots,RequestCpus,RequestDisk,RequestIoslots,RequestMemory,RequestRepackslots,DESIRED_Gatekeepers,DESIRED_Sites,DynamicSlot,MaxWallTimeMins,MachineLastMatchTime,Slot1_SelfMonitorAge,Slot1_TotalTimeClaimedBusy,ParentSlotId,REQUIRED_OS,Slot1_TotalTimeUnclaimedIdle,PartitionableSlot,CMSGroups,CRAB_UserHN,WMAgent_AgentName,ConcurrencyLimits,JobStatus,NiceUser,Rank,Requirements,RequestGPUs,GLIDEIN_USER,x509userproxysubject,x509userproxyfirstfqan,CMS_ALLOW_OVERFLOW,CRAB_UserRole,DESIRED_Overflow_Region,DESIRED_CMSDataset,D_FULLDEBUG,IS_TEST_JOB,HAS_SINGULARITY,OVERFLOW_IT,OVERFLOW_UK,OVERFLOW_US"
== JOB AD: x509UserProxyFQAN = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi,/cms/Role=NULL/Capability=NULL"
== JOB AD: RemoteWallClockTime = 0.0
== JOB AD: CRAB_Id = "1"
== JOB AD: JOB_GLIDEIN_ToRetire = "1525997417"
== JOB AD: Cmd = "/data/srv/glidecondor/condor_local/spool/4060/0/cluster20914060.proc0.subproc0/gWMS-CMSRunAnalysis.sh"
== JOB AD: MachineAttrHAS_SINGULARITY0 = true
== JOB AD: JOB_Gatekeeper = ifthenelse(substr(Used_Gatekeeper,0,1) =!= "$",Used_Gatekeeper,ifthenelse(MATCH_GLIDEIN_Gatekeeper =!= undefined,MATCH_GLIDEIN_Gatekeeper,"Unknown"))
== JOB AD: AccountingGroup = "analysis.wshi"
== JOB AD: JobCurrentStartDate = 1525774721
== JOB AD: use_x509userproxy = true
== JOB AD: DiskUsage = 4250
== JOB AD: CRAB_Retry = 2
== JOB AD: CRAB_Destination = "srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/DIGI_RAW/180508_093519/0000/log/cmsRun_1.log.tar.gz, srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/DIGI_RAW/180508_093519/0000/out_raw_1.root"
== JOB AD: ClusterId = 20916852
== JOB AD: CRAB_StageoutPolicy = "local,remote"
== JOB AD: RequestCpus = 1
== JOB AD: CondorVersion = "$CondorVersion: 8.6.8 Oct 30 2017 BuildID: 422919 $"
== JOB AD: CRAB_TaskWorker = "vocms052"
== JOB AD: JOB_GLIDEIN_Job_Max_Time = "14400"
== JOB AD: accounting_group = analysis
==== HTCONDOR JOB AD CONTENTS FINISH ====
======== HTCONDOR JOB SUMMARY at Tue May  8 10:18:48 GMT 2018 FINISH ========
======== PROXY INFORMATION START at Tue May  8 10:18:48 GMT 2018 ========
subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi/CN=1313364434/CN=659928051/CN=329744202/CN=1483302235/CN=745329382/CN=1075173255
issuer    : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi/CN=1313364434/CN=659928051/CN=329744202/CN=1483302235/CN=745329382
identity  : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi/CN=1313364434/CN=659928051/CN=329744202/CN=1483302235/CN=745329382
type      : RFC compliant proxy
strength  : 1024 bits
path      : /srv/aae00b45be2ec79d71339d457d5a9788729ac18a
timeleft  : 23:59:56
key usage : Digital Signature, Key Encipherment
=== VO cms extension information ===
VO        : cms
subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=wshi/CN=791920/CN=Wei Shi
issuer    : /DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch
attribute : /cms/Role=NULL/Capability=NULL
timeleft  : 167:17:13
uri       : lcg-voms2.cern.ch:15002
======== PROXY INFORMATION FINISH at Tue May  8 10:18:48 GMT 2018 ========
======== CMSRunAnalysis.sh at Tue May  8 10:18:48 GMT 2018 STARTING ========
======== CMSRunAnalysis.sh STARTING at Tue May  8 10:18:48 GMT 2018 ========
Local time : Tue May  8 10:18:48 UTC 2018
Current system : Linux c0121.brazos.tamu.edu 4.4.128-1.el6.elrepo.x86_64 #1 SMP Sat Apr 14 08:41:22 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
==== CMSSW pre-execution environment bootstrap STARTING ====
+ '[' -f /cmsset_default.sh ']'
+ '[' -f /home/osg/app/cmssoft/cms/cmsset_default.sh ']'
+ '[' -f /cvmfs/cms.cern.ch/cmsset_default.sh ']'
+ echo 'CVMFS style'
CVMFS style
+ export VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ set +x
+ declare -a VERSIONS
+ VERSIONS=($(ls /cvmfs/cms.cern.ch/$SCRAM_ARCH/external/python | egrep '2.[67]'))
++ egrep '2.[67]'
++ ls /cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python
+ PY_PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python
+ echo 'python version: ' 2.7.11
python version:  2.7.11
+ set +x
==== CMSSW pre-execution environment bootstrap FINISHING at Tue May  8 10:18:48 GMT 2018 ====
==== Python discovery STARTING ====
Python found in /cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11
I found python at..
/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11/bin/python
==== Python discovery FINISHING at Tue May  8 10:18:49 GMT 2018 ====
======== Current environment dump STARTING ========
== ENV: OSG_SINGULARITY_VERSION=2.5.0-dist
== ENV: LCMAPS_DB_FILE=/etc/lcmaps.db
== ENV: GLIDEIN_ResourceName=TAMU_BRAZOS_CE
== ENV: GLIDEIN_SEs=srm.brazos.tamu.edu
== ENV: SLURM_CHECKPOINT_IMAGE_DIR=/var/lib/slurm/checkpoint
== ENV: SLURM_NODELIST=c0121
== ENV: _CONDOR_JOB_PIDS=
== ENV: MANPATH=/cvmfs/cms.cern.ch/share/man:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/man:/apps/lmod/lmod/share/man::
== ENV: CMS_GLIDEIN_VERSION=10
== ENV: SLURM_JOB_NAME=GRIDJOB
== ENV: GRIDMAP=/tmp/SLURM_21302623_20158/glide_tHsSit/grid-mapfile
== ENV: SLURMD_NODENAME=c0121
== ENV: GLIDEIN_REQUIRE_GLEXEC_USE=False
== ENV: HOSTNAME=c0121.brazos.tamu.edu
== ENV: SLURM_TOPOLOGY_ADDR=c0121
== ENV: OSG_GLEXEC_LOCATION=/usr/local/bin/glexec
== ENV: REQUIRED_OS=rhel6
== ENV: SLURM_NODE_ALIASES=(null)
== ENV: OSG_SINGULARITY_BIND_GPU_LIBS=0
== ENV: GFAL_PLUGIN_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/gfal2-plugins/
== ENV: _CONDOR_ANCESTOR_32569=2708:1525774022:2659640729
== ENV: TERM=xterm
== ENV: VOMS_USERCONF=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/vomses
== ENV: LCMAPS_POLICY_NAME=authorize_only
== ENV: HISTSIZE=1000
== ENV: SLURM_JOB_QOS=hepx
== ENV: qos
== ENV: OSG_MACHINE_GPUS=0
== ENV: SLURM_TOPOLOGY_ADDR_PATTERN=node
== ENV: GLOBUS_LOCATION=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr
== ENV: LMOD_DEFAULT_MODULEPATH=/apps/modulefiles/Linux:/apps/modulefiles/Core:/apps/lmod/lmod/modulefiles/Core
== ENV: LMOD_SYSTEM_DEFAULT_MODULES=brazos
== ENV: TMPDIR=/srv
== ENV: MODULEPATH_ROOT=/apps/modulefiles
== ENV: PERL5LIB=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/perl5/vendor_perl:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/perl5:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/perl5/vendor_perl:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/perl5
== ENV: GLIDECLIENT_Signature=2148ef66640a2d6965b50dd6511f716b5a0a879b
== ENV: GAHP_TEMP=/tmp/condor_g_scratch.0x7f483bf1ffe0.1143867
== ENV: LMOD_PACKAGE_PATH=/apps/modulefiles/Site
== ENV: CVSROOT=:gserver:cmssw.cvs.cern.ch:/local/reps/CMSSW
== ENV: LMOD_PKG=/apps/lmod/6.0.15
== ENV: _CONDOR_SCRATCH_DIR=/srv
== ENV: HAS_CVMFS_singularity_opensciencegrid_org=True
== ENV: QTDIR=/usr/lib64/qt-3.3
== ENV: X509_CERT_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/grid-security/certificates
== ENV: CMS_VALIDATION_FRONTIER=0
== ENV: SCRAM_ARCH=slc6_amd64_gcc630
== ENV: LMOD_ADMIN_FILE=/apps/modulefiles/Site/admin.list
== ENV: OSG_WN_TMP=/tmp
== ENV: QTINC=/usr/lib64/qt-3.3/include
== ENV: LMOD_VERSION=6.0.15
== ENV: GLIDEIN_Max_Idle=600
== ENV: SCHEDD_NAME=ce01.brazos.tamu.edu
== ENV: OSG_SQUID_LOCATION=squid.brazos.tamu.edu:3128
== ENV: GLIDEIN_REQUIRE_VOMS=False
== ENV: GLEXEC_BIN=NONE
== ENV: NO_PROXY=.brazos.tamu.edu
== ENV: NODE_COUNT=1
== ENV: JOBSTARTDIR=/srv
== ENV: SLURM_NNODES=1
== ENV: http_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: USER=ws13
== ENV: _CHIRP_DELAYED_UPDATE_PREFIX=Chirp*
== ENV: OSG_SITE_WRITE=/fdata/scratch/osg
== ENV: CRAB_Retry=2
== ENV: LD_LIBRARY_PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/sqlite/3.15.1/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/libffi/3.2.1/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/gdbm/1.10/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/db6/6.0.30/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/expat/2.1.0/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/openssl/1.0.2d/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/bz2lib/1.0.6/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/zlib/1.2.8/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/gcc/6.3.0/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/gcc/6.3.0/lib:/srv/condor/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/lib64:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/dcap:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/lcgdm
== ENV: LMOD_sys=Linux
== ENV: CONDOR_PARENT_ID=c0121:8263:1525774723
== ENV: CMSSITE=T3_US_TAMU
== ENV: GLIDEIN_Entry_Name=CMS_T3_TAMU_BRAZOS_ce01
== ENV: OSG_SINGULARITY_IMAGE=/cvmfs/singularity.opensciencegrid.org/bbockelm/cms:rhel6
== ENV: VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
== ENV: GLIDEIN_Name=v3_2
== ENV: BATCH_SYSTEM=HTCondor
== ENV: OSG_LOCATION=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64
== ENV: SINGULARITY_NAME=86b5301ef4edf5ab73a46c6b412b31d1894ec1823cbc995afeb2a68e1faf8d
== ENV: ENV=/etc/profile
== ENV: OSG_SINGULARITY_IMAGE_DEFAULT=/cvmfs/singularity.opensciencegrid.org/bbockelm/cms:rhel6
== ENV: LLGT_LIFT_PRIVILEGED_PROTECTION=1
== ENV: CVMFS_oasis_opensciencegrid_org_TIMESTAMP=1525198991
== ENV: _CONDOR_CHIRP_CONFIG=/srv/.chirp.config
== ENV: SLURM_JOBID=21302623
== ENV: GLOBUS_GSSAPI_MAX_TLS_PROTOCOL=0
== ENV: FTP_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: ftp_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: GLIDEIN_Entry_Signature=d97277c2c7730292695037a14c007c24a6030d34
== ENV: CONDORCE_COLLECTOR_HOST=ce01.brazos.tamu.edu:9619
== ENV: OSG_SITE_READ=/fdata/scratch/osg
== ENV: LLGT_LOG_IDENT=htcondor-ce
== ENV: CONDOR_PROCD_ADDRESS_BASE=/tmp/SLURM_21302623_20158/glide_tHsSit/log/procd_address
== ENV: LMOD_PREPEND_BLOCK=normal
== ENV: _CONDOR_ANCESTOR_8263=8919:1525774728:4044490741
== ENV: GLIDEIN_Site=TAMU_BRAZOS
== ENV: GLIDECLIENT_Name=CMSG-v1_0.local_users
== ENV: OSG_SINGULARITY_REEXEC=1
== ENV: PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/python/2.7.11/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/sqlite/3.15.1/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/gdbm/1.10/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/db6/6.0.30/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/expat/2.1.0/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/openssl/1.0.2d/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/bz2lib/1.0.6/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/gcc/6.3.0/bin:/cvmfs/cms.cern.ch/common:/cvmfs/cms.cern.ch/bin:/srv/condor/libexec:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/bin:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin
== ENV: MAIL=/var/spool/mail/ws13
== ENV: SLURM_TASKS_PER_NODE=1
== ENV: _ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09MSxiYXNlTXBhdGhBPXsiL2FwcHMvbW9kdWxlZmlsZXMvTGludXgiLCIvYXBwcy9tb2R1bGVmaWxlcy9Db3JlIiwiL2FwcHMvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLH0sWyJjX3JlYnVpbGRUaW1lIl09ZmFsc2UsWyJjX3Nob3J0VGltZSJdPWZhbHNlLGZhbWlseT17fSxpbmFjdGl2ZT17fSxtVD17YnJhem9zPXtbIkZOIl09Ii9hcHBzL21vZHVsZWZpbGVzL0NvcmUvYnJhem9zLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImJyYXpvcyIsWyJsb2FkT3JkZXIiXT0xLHByb3BUPXtsbW9kPXtbInN0aWNreSJdPTEsfSx9LFsic2hvcnQiXT0iYnJhem9zIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sfSxtcGF0
== ENV: LCMAPS_DEBUG_LEVEL=2
== ENV: OSG_SINGULARITY_OUTSIDE_PWD=/tmp/SLURM_21302623_20158/glide_tHsSit/execute/dir_8263
== ENV: SLURM_CONF=/etc/slurm/slurm.conf
== ENV: HAS_CVMFS_oasis_opensciencegrid_org=True
== ENV: LCMAPS_DIR=/etc
== ENV: HOLD_GRACE_TIME=0
== ENV: CMS_PATH=/cvmfs/cms.cern.ch
== ENV: SLURM_JOB_ID=21302623
== ENV: CONDOR_CONFIG=/tmp/SLURM_21302623_20158/glide_tHsSit/condor_config
== ENV: OSG_DATA=/fdata/scratch/osg
== ENV: LMOD_SETTARG_CMD=:
== ENV: SLURM_JOB_USER=ws13
== ENV: OSG_APP=/home/osg/app
== ENV: PWD=/srv
== ENV: _LMFILES_=/apps/modulefiles/Core/brazos.lua
== ENV: CRAB_Id=1
== ENV: LMOD_PAGER=more
== ENV: LANG=C
== ENV: _CONDOR_WRAPPER_ERROR_FILE=/tmp/SLURM_21302623_20158/glide_tHsSit/execute/dir_8263/.job_wrapper_failure
== ENV: GLIDEIN_Country=US
== ENV: MODULEPATH=/apps/modulefiles/Linux:/apps/modulefiles/Core:/apps/lmod/lmod/modulefiles/Core
== ENV: TZ=UTC
== ENV: X509_VOMS_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/grid-security/vomsdir
== ENV: LOADEDMODULES=brazos
== ENV: _ModuleTable_Sz_=2
== ENV: GLIDEIN_Gatekeeper=ce01.brazos.tamu.edu
== ENV: ce01.brazos.tamu.edu:9619
== ENV: SLURM_JOB_UID=20158
== ENV: GATEKEEPER_JM_ID=2018-05-08.09:12:19.0001143867.0000000951
== ENV: GLIDEIN_Job_Max_Time=14400
== ENV: SLURM_NODEID=0
== ENV: _CONDOR_SLOT=
== ENV: OSG_GRID=/etc/osg/wn-client/
== ENV: CRAB_TASKMANAGER_TARBALL=local
== ENV: GLIDEIN_Tmp_Dir=/tmp/SLURM_21302623_20158/glide_tHsSit/tmp
== ENV: SLURM_SUBMIT_DIR=/work/condor/8549/0/cluster2458549.proc0.subproc0
== ENV: GLOBUS_TCP_PORT_RANGE_STATE_FILE=
== ENV: GLIDEIN_Proxy_URL=None
== ENV: SLURM_TASK_PID=29186
== ENV: GLIDEIN_CLAIM_WORKLIFE_DYNAMIC=cpus*(6*3600)
== ENV: LMOD_CMD=/apps/lmod/6.0.15/libexec/lmod
== ENV: GLIDEIN_Factory=CMS-CERN-Production
== ENV: GLIDEIN_GridType=condor
== ENV: OSG_SINGULARITY_PATH=/usr/bin/singularity
== ENV: https_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: HTTPS_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: GLIDECLIENT_Group=local_users
== ENV: LMOD_AVAIL_STYLE=grouped:system
== ENV: SLURM_CPUS_ON_NODE=1
== ENV: OSG_HOSTNAME=ce01.brazos.tamu.edu
== ENV: SLURM_PROCID=0
== ENV: HISTCONTROL=ignoredups
== ENV: ENVIRONMENT=BATCH
== ENV: SLURM_JOB_NODELIST=c0121
== ENV: _CONDOR_EXECUTE=/tmp/SLURM_21302623_20158/glide_tHsSit/execute
== ENV: GLIDEIN_Expose_Grid_Env=True
== ENV: GLIDECLIENT_Group_Signature=43f900294971967f4e826d823e02a377e7d8af62
== ENV: SHLVL=8
== ENV: HAS_CVMFS_cms_cern_ch=True
== ENV: GLOBUS_GSSAPI_NAME_COMPATIBILITY=STRICT_RFC2818
== ENV: HOME=/srv
== ENV: _CONDOR_ANCESTOR_2708=8263:1525774722:3224323620
== ENV: _CONDOR_MACHINE_AD=/srv/.machine.ad
== ENV: GLOBUS_TCP_PORT_RANGE=40000,41999
== ENV: OSG_STORAGE_ELEMENT=True
== ENV: GLIDEIN_Max_Tail=600
== ENV: SLURM_LOCALID=0
== ENV: X509_USER_PROXY=/srv/aae00b45be2ec79d71339d457d5a9788729ac18a
== ENV: GSI_AUTHZ_CONF=/dev/null
== ENV: java_options=-Xms128m
== ENV: -Xmx512m
== ENV: _ModuleTable002_=aEE9eyIvYXBwcy9tb2R1bGVmaWxlcy9MaW51eCIsIi9hcHBzL21vZHVsZWZpbGVzL0NvcmUiLCIvYXBwcy9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvYXBwcy9tb2R1bGVmaWxlcy9MaW51eDovYXBwcy9tb2R1bGVmaWxlcy9Db3JlOi9hcHBzL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIixbInZlcnNpb24iXT0yLH0=
== ENV: no_proxy=.brazos.tamu.edu
== ENV: SLURM_CLUSTER_NAME=brazos
== ENV: SLURM_JOB_GID=2015
== ENV: JOB_REPOSITORY_ID=2018-05-08.09:12:19.0001143867.0000000951
== ENV: GLOBUS_GSSAPI_MIN_TLS_PROTOCOL=TLS1_VERSION
== ENV: SLURM_JOB_CPUS_PER_NODE=1
== ENV: SLOTS_LAYOUT=fixed
== ENV: GFAL_CONFIG_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/gfal2.d/
== ENV: GLIDEIN_CMSSite=T3_US_TAMU
== ENV: SLURM_SUBMIT_HOST=ce01.brazos.tamu.edu
== ENV: HTTP_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: CVMFS_oasis_opensciencegrid_org_REVISION=9099
== ENV: GLIDEIN_Signature=b5a1e623f9556c0898d245397da0d07f2b8b5d10
== ENV: SLURM_GTIDS=0
== ENV: GLIDEIN_Schedd=schedd_glideins5@vocms0805.cern.ch
== ENV: SLURM_JOB_PARTITION=stakeholder
== ENV: OSG_DEFAULT_SE=srm.brazos.tamu.edu
== ENV: PYTHONPATH=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
== ENV: LMOD_arch=x86_64
== ENV: LOGNAME=ws13
== ENV: GLIDEIN_Glexec_Use=OPTIONAL
== ENV: LESS=-XqeMRF
== ENV: GLIDEIN_REQUIRED_OS=any
== ENV: GPUDetection=No
== ENV: GPUs
== ENV: detected
== ENV: CVMFS_cms_cern_ch_REVISION=54165
== ENV: QTLIB=/usr/lib64/qt-3.3/lib
== ENV: HAS_SINGULARITY=1
== ENV: CVS_RSH=ssh
== ENV: OSG_SITE_NAME=TAMU_BRAZOS_CE
== ENV: SLURM_JOB_ACCOUNT=hepx
== ENV: GLIDEIN_Monitoring_Enabled=False
== ENV: MODULESHOME=/apps/lmod/6.0.15
== ENV: GLIDEIN_ClusterId=1255353
== ENV: GLIDEIN_ProcId=7
== ENV: SLURM_JOB_NUM_NODES=1
== ENV: OMP_NUM_THREADS=1
== ENV: GLOBUS_GSSAPI_CIPHERS=HIGH
== ENV: GLOBUS_GSSAPI_BACKWARD_COMPATIBLE_MIC=true
== ENV: LESSOPEN=||/usr/bin/lesspipe.sh
== ENV: %s
== ENV: __Init_Default_Modules=1
== ENV: _CONDOR_JOB_AD=/srv/.job.ad
== ENV: SINGULARITY_CONTAINER=86b5301ef4edf5ab73a46c6b412b31d1894ec1823cbc995afeb2a68e1faf8d
== ENV: LMOD_FULL_SETTARG_SUPPORT=no
== ENV: OSG_SINGULARITY_AUTOLOAD=1
== ENV: GLITE_LOCATION=/usr/libexec/condor/glite
== ENV: CVMFS_singularity_opensciencegrid_org_REVISION=34308
== ENV: CRAB_RUNTIME_TARBALL=local
== ENV: GLOBUS_GSSAPI_FORCE_TLS=false
== ENV: GLIDEIN_LOCAL_TMP_DIR=/tmp/glide_ws13_uzYM21
== ENV: LMOD_DIR=/apps/lmod/6.0.15/libexec
== ENV: _CONDOR_JOB_IWD=/srv
== ENV: OSG_SINGULARITY_BIND_CVMFS=1
== ENV: SLURM_MEM_PER_NODE=1900
== ENV: G_BROKEN_FILENAMES=1
== ENV: LMOD_COLORIZE=yes
== ENV: SCRATCH=/fdata/scratch/ws13
== ENV: USER_DN=/DC=ch/DC=cern/OU=Organic
== ENV: Units/OU=Users/CN=wshi/CN=791920/CN=Wei
== ENV: Shi
== ENV: CONDOR_PROCD_ADDRESS=/tmp/SLURM_21302623_20158/glide_tHsSit/log/procd_address
== ENV: GLOBUS_GSSAPI_SERVER_CIPHER_ORDER=true
== ENV: _=/bin/env
======== Current environment dump FINISHING ========
======== Tarball initialization STARTING at Tue May  8 10:18:49 GMT 2018 ========
+ [[ X == \X ]]
+ [[ local == \l\o\c\a\l ]]
+ tar xzmf CMSRunAnalysis.tar.gz
++ pwd
++ pwd
+ export PYTHONPATH=/srv/CRAB3.zip:/srv/WMCore.zip:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
+ PYTHONPATH=/srv/CRAB3.zip:/srv/WMCore.zip:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
+ set +x
======== Tarball initialization FINISHING at Tue May  8 10:18:49 GMT 2018 ========
==== Local directory contents dump STARTING ====
PWD: /srv
== DIR: ApmonIf.py
== DIR: CMSGroupMapper.py
== DIR: CMSRunAnalysis.py
== DIR: CMSRunAnalysis.sh
== DIR: CMSRunAnalysis.tar.gz
== DIR: DashboardAPI.py
== DIR: DashboardFailure.sh
== DIR: Logger.py
== DIR: ProcInfo.py
== DIR: RESTInteractions.py
== DIR: ServerUtilities.py
== DIR: TweakPSet.py
== DIR: WMArchiveReport.json
== DIR: WMArchiveReport.json.1
== DIR: WMCore.zip
== DIR: _condor_stderr
== DIR: _condor_stdout
== DIR: aae00b45be2ec79d71339d457d5a9788729ac18a
== DIR: apmon.py
== DIR: cmscp.py
== DIR: condor
== DIR: condor_exec.exe
== DIR: input_files.tar.gz
== DIR: jobReport.json
== DIR: jobReport.json.1
== DIR: libcurl.so.4
== DIR: run_and_lumis.tar.gz
== DIR: sandbox.tar.gz
== DIR: startup_environment.sh
==== Local directory contents dump FINISHING ====
======== CMSRunAnalysis.py STARTING at Tue May  8 10:18:49 GMT 2018 ========
Now running the CMSRunAnalysis.py job in /srv...
++ pwd
+ python CMSRunAnalysis.py -r /srv -a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=1 --cmsswVersion=CMSSW_9_4_1 --scramArch=slc6_amd64_gcc630 --inputFile=job_input_file_list_1.txt --runAndLumis=job_lumis_1.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 '--scriptArgs=[]' -o '{}' --oneEventMode=0
==== CMSRunAnalysis.py STARTING at Tue May  8 10:18:49 2018 ====
Local time : Tue May  8 10:18:49 2018
Dashboard early startup params: {'MonitorID': '180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100', 'MonitorJobID': '1_https://glidein.cern.ch/1/180508:093519:wshi:crab:DIGI:RAW:DarkSUSY:mH:125:mN1:10:mND:1:mGammaD:0p25:cT:100_2', 'SyncCE': 'ce01.brazos.tamu.edu', 'OverflowFlag': 0, 'SyncSite': 'T3_US_TAMU', 'SyncGridJobId': 'https://glidein.cern.ch/1/180508:093519:wshi:crab:DIGI:RAW:DarkSUSY:mH:125:mN1:10:mND:1:mGammaD:0p25:cT:100', 'WNHostName': 'c0121.brazos.tamu.edu'}
==== Parameters Dump at Tue May  8 10:18:49 2018 ===
archiveJob:     sandbox.tar.gz
runDir:         /srv
sourceURL:      https://cmsweb.cern.ch/crabcache
jobNumber:      1
cmsswVersion:   CMSSW_9_4_1
scramArch:      slc6_amd64_gcc630
inputFile       job_input_file_list_1.txt
outFiles:       {}
runAndLumis:    job_lumis_1.json
lheInputFiles:  False
firstEvent:     None
firstLumi:      None
eventsPerLumi:  None
lastEvent:      None
firstRun:       None
seeding:        AutomaticSeeding
userFiles:      None
oneEventMode:   0
scriptExe:      None
scriptArgs:     []
maxRuntime:     -60
===================
==== Sandbox preparation STARTING at Tue May  8 10:18:49 2018 ====
Sandbox sandbox.tar.gz already exists, skipping

==== Sandbox preparation FINISHING at Tue May  8 10:18:49 2018 ====
==== WMCore filesystem preparation STARTING at Tue May  8 10:18:49 2018 ====
==== WMCore filesystem preparation FINISHING at Tue May  8 10:18:49 2018 ====
Dashboard startup parameters: {'MonitorID': '180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100', 'MonitorJobID': '1_https://glidein.cern.ch/1/180508:093519:wshi:crab:DIGI:RAW:DarkSUSY:mH:125:mN1:10:mND:1:mGammaD:0p25:cT:100_2', 'WNHostName': 'c0121.brazos.tamu.edu', 'ExeStart': 'cmsRun'}
==== CMSSW Stack Execution STARTING at Tue May  8 10:18:49 2018 ====
==== SCRAM Obj CREATED at Tue May  8 10:18:49 2018 ====

==== CMSSW JOB Execution started at Tue May  8 10:18:51 2018 ====
2018-05-08 10:18:51,346:INFO:Scram:    Invoking command: python -c 'from PSetTweaks.WMTweak import makeTweak;config = __import__("WMTaskSpace.cmsRun.PSet", globals(), locals(), ["process"], -1);tweakJson = makeTweak(config.process).jsondictionary();print tweakJson["process"]["outputModules_"]'
2018-05-08 10:18:57,155:INFO:CMSSW:User files are 
2018-05-08 10:18:57,155:INFO:CMSSW:User sandboxes are sandbox.tar.gz
2018-05-08 10:18:57,156:INFO:CMSSW:CMSSW configured for 1 cores and 0 event streams
2018-05-08 10:18:57,156:INFO:CMSSW:Executing CMSSW step
2018-05-08 10:18:57,156:INFO:CMSSW:Runing SCRAM
2018-05-08 10:18:57,958:INFO:CMSSW:Running PRE scripts
2018-05-08 10:18:57,959:INFO:CMSSW:RUNNING SCRAM SCRIPTS
2018-05-08 10:18:57,964:INFO:Scram:    Invoking command: python /srv/TweakPSet.py --location=/srv --inputFile='job_input_file_list_1.txt' --runAndLumis='job_lumis_1.json' --firstEvent=None --lastEvent=None --firstLumi=None --firstRun=None --seeding=AutomaticSeeding --lheInputFiles=False --oneEventMode=0 --eventsPerLumi=None --maxRuntime=-60 

2018-05-08 10:19:13,966:INFO:CMSSW:Executing CMSSW. args: ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc630', 'scramv1', 'CMSSW', 'CMSSW_9_4_1', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']
2018-05-08 10:19:34,124:CRITICAL:CMSSW:Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc630', 'scramv1', 'CMSSW', 'CMSSW_9_4_1', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

==== CMSSW Stack Execution FAILED at Tue May  8 10:19:34 2018 ====
======== CMSSW OUTPUT STARTING ========
NOTICE: lines longer than 3000 characters will be truncated
== CMSSW: Beginning CMSSW wrapper script
== CMSSW:  slc6_amd64_gcc630 scramv1 CMSSW
== CMSSW: Performing SCRAM setup...
== CMSSW: Completed SCRAM setup
== CMSSW: Retrieving SCRAM project...
== CMSSW: Untarring  /srv/sandbox.tar.gz
== CMSSW: Completed SCRAM project
== CMSSW: Executing CMSSW
== CMSSW: cmsRun  -j FrameworkJobReport.xml PSet.py
== CMSSW: 08-May-2018 10:19:30 UTC  Initiating request to open file /fdata/hepx/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root
== CMSSW: 08-May-2018 10:19:31 UTC  Fallback request to file root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root
== CMSSW: %MSG-w XrdAdaptorInternal:  file_open 08-May-2018 10:19:33 UTC pre-events
== CMSSW: Failed to open file at URL root://cms-xrd-global.cern.ch:1094//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root?tried=+1213xrootd.ba.infn.it.
== CMSSW: %MSG
== CMSSW: %MSG-w XrdAdaptorInternal:  file_open 08-May-2018 10:19:34 UTC pre-events
== CMSSW: Failed to open file at URL root://cms-xrd-global.cern.ch:1094//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root?tried=+1213xrootd.ba.infn.it,.
== CMSSW: %MSG
== CMSSW: ----- Begin Fatal Exception 08-May-2018 10:19:34 UTC-----------------------
== CMSSW: An exception of category 'FallbackFileOpenError' occurred while
== CMSSW:    [0] Constructing the EventProcessor
== CMSSW:    [1] Constructing input source of type PoolSource
== CMSSW:    [2] Calling RootFileSequenceBase::initTheFile()
== CMSSW:    [3] Calling StorageFactory::open()
== CMSSW:    [4] Calling XrdFile::open()
== CMSSW: Exception Message:
== CMSSW: Failed to open the file 'root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root'
== CMSSW:    Additional Info:
== CMSSW:       [a] open() failed with system error 'No such file or directory' (error code 2)
== CMSSW:       [b] Input file /fdata/hepx/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root could not be opened.
== CMSSW: Fallback Input file root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root also could not be opened.
== CMSSW: Original exception info is above; fallback exception info is below.
== CMSSW:       [c] XrdCl::File::Open(name='root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root', flags=0x10, permissions=0660) => error '[ERROR] Server responded with an error: [3011] No servers are available to read the file.
== CMSSW: ' (errno=3011, code=400). No additional data servers were found.
== CMSSW:       [d] Last URL tried: root://cms-xrd-global.cern.ch:1094//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root?tried=+1213xrootd.ba.infn.it,
== CMSSW:       [e] Problematic data server: cms-xrd-global.cern.ch:1094
== CMSSW:       [f] Disabled source: cms-xrd-global.cern.ch:1094
== CMSSW: ----- End Fatal Exception -------------------------------------------------
== CMSSW: Complete
== CMSSW: process id is 266 status is 92
======== CMSSW OUTPUT FINSHING ========
ERROR: Caught WMExecutionFailure - code = 92 - name = CmsRunFailure - detail = Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc630', 'scramv1', 'CMSSW', 'CMSSW_9_4_1', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

NOTE: FJR has exit code 8028 and WMCore reports 92; preferring the FJR one.
ERROR: Exceptional exit at Tue May  8 10:19:34 2018 (8028): CmsRunFailure
CMSSW error message follows.
Fatal Exception
An exception of category 'FallbackFileOpenError' occurred while
   [0] Constructing the EventProcessor
   [1] Constructing input source of type PoolSource
   [2] Calling RootFileSequenceBase::initTheFile()
   [3] Calling StorageFactory::open()
   [4] Calling XrdFile::open()
Exception Message:
Failed to open the file 'root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root'
   Additional Info:
      [a] open() failed with system error 'No such file or directory' (error code 2)
      [b] Input file /fdata/hepx/store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root could not be opened.
Fallback Input file root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root also could not be opened.
Original exception info is above; fallback exception info is below.
      [c] XrdCl::File::Open(name='root://xrootd.ba.infn.it//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root', flags=0x10, permissions=0660) => error '[ERROR] Server responded with an error: [3011] No servers are available to read the file.
' (errno=3011, code=400). No additional data servers were found.
      [d] Last URL tried: root://cms-xrd-global.cern.ch:1094//store/user/wshi/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/GEN_SIM/180507_195402/0000/out_sim_1.root?tried=+1213xrootd.ba.infn.it,
      [e] Problematic data server: cms-xrd-global.cern.ch:1094
      [f] Disabled source: cms-xrd-global.cern.ch:1094

ERROR: Traceback follows:
 Traceback (most recent call last):
  File "CMSRunAnalysis.py", line 903, in <module>
    cmssw = executeCMSSWStack(opts, scram)
  File "CMSRunAnalysis.py", line 701, in executeCMSSWStack
    cmssw.execute()
  File "/srv/WMCore.zip/WMCore/WMSpec/Steps/Executors/CMSSW.py", line 251, in execute
    raise WMExecutionFailure(returncode, "CmsRunFailure", msg)
WMExecutionFailure: CmsRunFailure
Message: Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc630', 'scramv1', 'CMSSW', 'CMSSW_9_4_1', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

	ModuleName : WMCore.WMSpec.Steps.WMExecutionFailure
	MethodName : __init__
	ClassInstance : None
	FileName : /srv/WMCore.zip/WMCore/WMSpec/Steps/WMExecutionFailure.py
	ClassName : None
	LineNumber : 18
	ErrorNr : 92

Traceback: 



== Execution site for failed job from site-local-config.xml: T3_US_TAMU
Dashboard end parameters: {'MonitorID': '180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100', 'MonitorJobID': '1_https://glidein.cern.ch/1/180508:093519:wshi:crab:DIGI:RAW:DarkSUSY:mH:125:mN1:10:mND:1:mGammaD:0p25:cT:100_2', 'NEventsProcessed': 0, 'JobExitCode': 8028, 'NCores': 1, 'ExeExitCode': 8028}
Not sending data to popularity service because no input sources found.
Dashboard popularity report: {'Basename': '', 'inputFiles': '', 'BasenameParent': '', 'inputBlocks': '/DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100_MG5_pythia8/wshi-GEN_SIM-21fd64682e845e630b293e515db358b1/USER#8b0ce5c3-1766-4903-8c5d-2c7fc04a403a', 'parentFiles': ''}
+ jobrc=92
+ set +x
== The job had an exit code of 92 
======== CMSRunAnalysis.py FINISHING at Tue May  8 10:19:35 GMT 2018 ========
==== SCRAM interaction log contents dump STARTING ====
Log for recording SCRAM command-line output
-------------------------------------------
Beginning TweakPSet
 arguments: ['/srv/TweakPSet.py', '--location=/srv', '--inputFile=job_input_file_list_1.txt', '--runAndLumis=job_lumis_1.json', '--firstEvent=None', '--lastEvent=None', '--firstLumi=None', '--firstRun=None', '--seeding=AutomaticSeeding', '--lheInputFiles=False', '--oneEventMode=0', '--eventsPerLumi=None', '--maxRuntime=-60']
One event mode disabled until we can put together a decent version of WMCore.
TweakPSet.py is going to force one event mode
Tag chirp updates from CMSSW with _cmsRun_
==== SCRAM interaction log contents dump FINISHING ====

real	0m46.293s
user	0m34.763s
sys	0m3.604s
CMSRunAnalysis.sh complete at Tue May  8 10:19:35 GMT 2018 with (short) exit status 92
======== CMSRunAnalsysis.sh at Tue May  8 10:19:35 GMT 2018 FINISHING ========
======== python2.6 bootstrap for stageout at Tue May  8 10:19:35 GMT 2018 STARTING ========
+ '[' -f /cmsset_default.sh ']'
+ '[' -f /home/osg/app/cmssoft/cms/cmsset_default.sh ']'
+ '[' -f /cvmfs/cms.cern.ch/cmsset_default.sh ']'
+ export VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ export CMS_PATH=/cvmfs/cms.cern.ch
+ CMS_PATH=/cvmfs/cms.cern.ch
+ set +x
+ '[' -e /cvmfs/cms.cern.ch/COMP/slc6_amd64_gcc493/external/python/2.7.6/etc/profile.d/init.sh ']'
+ set +x
+ command -v python2.7
+ rc=0
+ set +x
Found python2.7 at:
/cvmfs/cms.cern.ch/COMP/slc6_amd64_gcc493/external/python/2.7.6/bin/python2.7
======== python2.7 bootstrap for stageout at Tue May  8 10:19:35 GMT 2018 FINISHING ========
======== Attempting to notify HTCondor of file stageout ========
	Error: 22 (Invalid argument)
======== Stageout at Tue May  8 10:19:35 GMT 2018 STARTING ========
====== Tue May  8 10:19:35 2018: cmscp.py STARTING.
The user has not specified to transfer the log files. No log files stageout (nor log files metadata upload) will be performed.
Stageout policy: local, remote
====== Tue May  8 10:19:35 2018: Starting job report validation.
Job report seems ok (it has the expected structure).
Retrieved payload exit code ('jobExitCode') = 8028 from job report.
Retrieved job wrapper exit code ('exitCode') = 8028 from job report.
====== Tue May  8 10:19:35 2018: Finished job report validation (status 0).
====== Tue May  8 10:19:35 2018: Starting to check if user output files exist.
ERROR: Output file out_raw.root does not exist.
====== Tue May  8 10:19:35 2018: Finished to check if user output files exist (status 60302).
Setting stageout wrapper exit info to {'exit_acronym': 'FAILED', 'exit_code': 60302, 'exit_msg': 'CmsCpFailure\ncmscp error message follows.\nERROR: Output file out_raw.root does not exist.'}.
Will use gfal2 commands for direct stageout.
Job wrapper did not finish successfully (exit code 8028). Setting that same exit code for the stageout wrapper.
Stageout wrapper finished with exit code 8028. Will report failure to Dashboard.
Dashboard stageout failure parameters: {'MonitorID': '180508_093519:wshi_crab_DIGI_RAW_DarkSUSY_mH_125_mN1_10_mND_1_mGammaD_0p25_cT_100', 'MonitorJobID': '1_https://glidein.cern.ch/1/180508:093519:wshi:crab:DIGI:RAW:DarkSUSY:mH:125:mN1:10:mND:1:mGammaD:0p25:cT:100_2', 'JobExitCode': 8028}
====== Tue May  8 10:19:36 2018: cmscp.py FINISHING (status 8028).
======== Stageout at Tue May  8 10:19:36 GMT 2018 FINISHING (short status 92) ========
======== gWMS-CMSRunAnalysis.sh FINISHING at Tue May  8 10:19:36 GMT 2018 on c0121.brazos.tamu.edu with (short) status 92 ========
Local time: Tue May  8 10:19:36 UTC 2018
Short exit status: 92
======== Figuring out long exit code of the job for condor_chirp ========
==== Long exit code of the job is 8028 ====
======== Finished condor_chirp -ing the exit code of the job. Exit code of condor_chirp: 0 ========
Job Running time in seconds:  48
Job runtime is less than 1 minute. Sleeping  12
