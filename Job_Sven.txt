WARN $TMPDIR(/tmp/SLURM_21303153_1507/glide_clU8RS/execute/dir_6236) is not a writable directory setting $TMPDIR = $PWD
======== gWMS-CMSRunAnalysis.sh STARTING at Tue May  8 17:38:52 GMT 2018 on c0112.brazos.tamu.edu ========
Local time : Tue May  8 17:38:52 UTC 2018
Current system : Linux c0112.brazos.tamu.edu 4.4.128-1.el6.elrepo.x86_64 #1 SMP Sat Apr 14 08:41:22 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
Arguments are -a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=40 --cmsswVersion=CMSSW_8_0_30 --scramArch=slc6_amd64_gcc530 --inputFile=job_input_file_list_40.txt --runAndLumis=job_lumis_40.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 --scriptArgs=[] -o {}
SCRAM_ARCH=slc6_amd64_gcc530
======== HTCONDOR JOB SUMMARY at Tue May  8 17:38:52 GMT 2018 START ========
CRAB ID: 40
Execution site: T3_US_TAMU
Current hostname: c0112.brazos.tamu.edu
Output files: out_raw.root=out_raw_40.root
==== HTCONDOR JOB AD CONTENTS START ====
== JOB AD: ProvisionedResources = "Cpus Memory Disk Swap"
== JOB AD: CRAB_PrimaryDataset = "DarkSUSY_mH_125_mGammaD_0250_13TeV"
== JOB AD: CumulativeRemoteUserCpu = 0.0
== JOB AD: RequestMemory = 2000
== JOB AD: CMS_ALLOW_OVERFLOW = "True"
== JOB AD: TransferOutput = "jobReport.json.40,WMArchiveReport.json.40"
== JOB AD: JobStatus = 2
== JOB AD: CRAB_TaskEndTime = 1528378353
== JOB AD: CoreSize = -1
== JOB AD: JOB_GLIDEIN_SiteWMS = "SLURM"
== JOB AD: DESIRED_Archs = "X86_64"
== JOB AD: JOB_GLIDECLIENT_Name = "CMSG-v1_0.local_users"
== JOB AD: JOB_GLIDEIN_Entry_Name = "CMS_T3_TAMU_BRAZOS_ce01"
== JOB AD: Used_Gatekeeper = "ce01.brazos.tamu.edu ce01.brazos.tamu.edu:9619"
== JOB AD: EncryptExecuteDirectory = false
== JOB AD: JOB_Site = "TAMU_BRAZOS"
== JOB AD: StartdPrincipal = "execute-side@matchsession/165.91.55.24"
== JOB AD: CRAB_SiteBlacklist = {  }
== JOB AD: JOB_GLIDEIN_Schedd = "schedd_glideins5@vocms0805.cern.ch"
== JOB AD: TargetType = "Machine"
== JOB AD: NiceUser = false
== JOB AD: ShadowBday = 1525801122
== JOB AD: DESIRED_Overflow_Region = strcat(ifthenelse(OVERFLOW_US =?= "True","US","none"),",",ifthenelse(OVERFLOW_IT =?= "True","IT","none"),",",ifthenelse(OVERFLOW_UK =?= "True","UK","none"))
== JOB AD: TransferIn = false
== JOB AD: NumCkpts_RAW = 0
== JOB AD: ExecutableSize = 10
== JOB AD: JobRunCount = 1
== JOB AD: CRAB_oneEventMode = 0
== JOB AD: JOB_GLIDEIN_MaxMemMBs = "2500"
== JOB AD: CRAB_RestURInoAPI = "/crabserver/prod"
== JOB AD: JOB_GLIDEIN_ClusterId = "1256895"
== JOB AD: CommittedSlotTime = 0
== JOB AD: PeriodicRelease = ( HoldReasonCode == 28 ) || ( HoldReasonCode == 30 ) || ( HoldReasonCode == 13 ) || ( HoldReasonCode == 6 )
== JOB AD: User = "cms911@cms"
== JOB AD: CRAB_RestHost = "cmsweb.cern.ch"
== JOB AD: DiskProvisioned = 77620080
== JOB AD: ExecutableSize_RAW = 9
== JOB AD: CRAB_OutLFNDir = "/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232"
== JOB AD: x509UserProxyFirstFQAN = "/cms/Role=NULL/Capability=NULL"
== JOB AD: NumRestarts = 0
== JOB AD: JOB_GLIDEIN_Site = "TAMU_BRAZOS"
== JOB AD: TotalSubmitProcs = 1
== JOB AD: LastJobLeaseRenewal = 1525801123
== JOB AD: SubmitEventNotes = "DAG Node: Job40"
== JOB AD: DAGParentNodeNames = ""
== JOB AD: CRAB_RetryOnASOFailures = 1
== JOB AD: JobLeaseDuration = 2400
== JOB AD: CRAB_ASODB = "filetransfers"
== JOB AD: Requirements = ( ( ( target.IS_GLIDEIN =!= true ) || ( target.GLIDEIN_CMSSite =!= undefined ) ) ) && ( TARGET.Arch == "X86_64" ) && ( TARGET.OpSys == "LINUX" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer )
== JOB AD: ShadowIpAddr = "<188.184.88.236:4080?addrs=188.184.88.236-4080&noUDP&sock=3215387_c4da_668355>"
== JOB AD: LocalSysCpu = 0.0
== JOB AD: x509UserProxyEmail = "sven.dildick@cern.ch"
== JOB AD: Arguments = "-a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=40 --cmsswVersion=CMSSW_8_0_30 --scramArch=slc6_amd64_gcc530 --inputFile=job_input_file_list_40.txt --runAndLumis=job_lumis_40.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 --scriptArgs=[] -o {}"
== JOB AD: PeriodicRemove = ( ( JobStatus =?= 5 ) && ( time() - EnteredCurrentStatus > 7 * 60 ) ) || ( ( JobStatus =?= 1 ) && ( time() - EnteredCurrentStatus > 7 * 24 * 60 * 60 ) ) || ( ( JobStatus =?= 2 ) && ( ( MemoryUsage > RequestMemory ) || ( MaxWallTimeMins * 60 < time() - EnteredCurrentStatus ) || ( DiskUsage > 20000000 ) ) ) || ( time() > CRAB_TaskEndTime ) || ( ( JobStatus =?= 1 ) && ( time() > ( x509UserProxyExpiration + 86400 ) ) )
== JOB AD: MachineAttrTotalSlotCpus0 = 1
== JOB AD: JobStartDate = 1525801122
== JOB AD: RemoteHost = "glidein_24623_66977532@c0112.brazos.tamu.edu"
== JOB AD: OrigIwd = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0"
== JOB AD: CRAB_UserGroup = undefined
== JOB AD: PostJobPrio1 = -1525786405
== JOB AD: EstimatedWallTimeMins = 1250
== JOB AD: Err = "_condor_stderr"
== JOB AD: PostJobPrio2 = 5
== JOB AD: ShadowVersion = "$CondorVersion: 8.6.8 Oct 30 2017 BuildID: 422919 $"
== JOB AD: NumSystemHolds = 0
== JOB AD: GlobalJobId = "crab3@vocms0121.cern.ch#22994598.0#1525801111"
== JOB AD: JOBGLIDEIN_CMSSite = "T3_US_TAMU"
== JOB AD: PublicClaimId = "<192.168.200.21:36338>#1525800304#1#..."
== JOB AD: Environment = "CRAB_TASKMANAGER_TARBALL=local SCRAM_ARCH=slc6_amd64_gcc530 CRAB_RUNTIME_TARBALL=local"
== JOB AD: CRAB_UserDN = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick"
== JOB AD: RequestMemory_RAW = 2000
== JOB AD: PeriodicHold = false
== JOB AD: ProcId = 0
== JOB AD: CRAB_PublishName = "crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT-00000000000000000000000000000000"
== JOB AD: StartdIpAddr = "<192.168.200.21:36338?CCBID=131.225.205.232:9626%3faddrs%3d131.225.205.232-9626#3363771%20188.184.83.197:9626%3faddrs%3d188.184.83.197-9626#6009954&addrs=192.168.200.21-36338+[--1]-36338&noUDP>"
== JOB AD: x509UserProxyVOName = "cms"
== JOB AD: OnExitHold = false
== JOB AD: x509userproxysubject = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick"
== JOB AD: CRAB_DataBlock = "/DarkSUSY_mH_125_mGammaD_0250_13TeV/dildick-DarkSUSY_mH_125_mGammaD_0250_13TeV-56b8d97cfc2d79b4eb9b54680fe363d1/USER#cf9a8571-274d-4a22-b465-a7db5e1cd5ae"
== JOB AD: TotalSuspensions = 0
== JOB AD: LeaveJobInQueue = false
== JOB AD: REQUIRED_OS = "rhel6"
== JOB AD: CMSGroups = "/cms,T3_US_TAMU,/cms/uscms,/cms/becms"
== JOB AD: CRAB_ISB = "https://cmsweb.cern.ch/crabcache"
== JOB AD: OrigMaxHosts = 1
== JOB AD: CRAB_AsyncDest = "T3_US_TAMU"
== JOB AD: NumCkpts = 0
== JOB AD: DAGNodeName = "Job40"
== JOB AD: Out = "_condor_stdout"
== JOB AD: NumJobCompletions = 0
== JOB AD: CRAB_UserHN = "dildick"
== JOB AD: AcctGroupUser = "dildick"
== JOB AD: JobPrio = 10
== JOB AD: CRAB_TaskLifetimeDays = 30
== JOB AD: CRAB_PublishGroupName = 0
== JOB AD: WantRemoteIO = true
== JOB AD: RootDir = "/"
== JOB AD: DESIRED_CMSDataset = "/DarkSUSY_mH_125_mGammaD_0250_13TeV/dildick-DarkSUSY_mH_125_mGammaD_0250_13TeV-56b8d97cfc2d79b4eb9b54680fe363d1/USER"
== JOB AD: WantCheckpoint = false
== JOB AD: OrigCmd = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0/gWMS-CMSRunAnalysis.sh"
== JOB AD: JOB_GLIDEIN_Factory = "CMS-CERN-Production"
== JOB AD: MachineAttrMJF_JOB_HS06_JOB0 = "Unknown"
== JOB AD: CpusProvisioned = 1
== JOB AD: RequestDisk_RAW = 1
== JOB AD: JOB_GLIDEIN_Memory = "2500"
== JOB AD: WhenToTransferOutput = "ON_EXIT_OR_EVICT"
== JOB AD: CRAB_AdditionalOutputFiles = {  }
== JOB AD: ExitStatus = 0
== JOB AD: MachineAttrDIRACBenchmark0 = 8.21287779252
== JOB AD: CurrentHosts = 1
== JOB AD: BufferSize = 524288
== JOB AD: CumulativeRemoteSysCpu = 0.0
== JOB AD: CRAB_PublishDBSURL = "https://cmsweb.cern.ch/dbs/prod/phys03/DBSWriter"
== JOB AD: NumJobStarts = 0
== JOB AD: CRAB_OutTempLFNDir = "/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232"
== JOB AD: MaxWallTimeMins_RAW = 1250
== JOB AD: LastSuspensionTime = 0
== JOB AD: MaxHosts = 1
== JOB AD: CRAB_NumAutomJobRetries = 2
== JOB AD: OVERFLOW_IT = ifthenelse(regexp("T[1,2]_IT_",DESIRED_Sites),"True",undefined)
== JOB AD: MinHosts = 1
== JOB AD: JOB_GLIDEIN_SiteWMS_Slot = "Unknown"
== JOB AD: CRAB_JobSW = "CMSSW_8_0_30"
== JOB AD: UidDomain = "cms"
== JOB AD: Owner = "cms911"
== JOB AD: DelegatedProxyExpiration = 1525887523
== JOB AD: ShouldTransferFiles = "YES"
== JOB AD: CRAB_JobType = "analysis"
== JOB AD: CRAB_SaveLogsFlag = 1
== JOB AD: ExitBySignal = false
== JOB AD: JobAdInformationAttrs = "MATCH_EXP_JOBGLIDEIN_CMSSite, JOBGLIDEIN_CMSSite, RemoteSysCpu, RemoteUserCpu"
== JOB AD: WantRemoteSyscalls = false
== JOB AD: CRAB_ASOTimeout = 86400
== JOB AD: CompletionDate = 0
== JOB AD: TransferInput = "CMSRunAnalysis.sh,cmscp.py,CMSRunAnalysis.tar.gz,sandbox.tar.gz,run_and_lumis.tar.gz,input_files.tar.gz"
== JOB AD: CumulativeSuspensionTime = 0
== JOB AD: JOB_GLIDEIN_ToDie = "1526042272"
== JOB AD: JOB_GLIDEIN_ProcId = "8"
== JOB AD: In = "/dev/null"
== JOB AD: RemoteSlotID = 1
== JOB AD: CRAB_UserRole = undefined
== JOB AD: MyType = "Job"
== JOB AD: DiskUsage_RAW = 9206
== JOB AD: JOB_GLIDEIN_Name = "v3_2"
== JOB AD: CRAB_Publish = 1
== JOB AD: JOB_GLIDEIN_Max_Walltime = "432000"
== JOB AD: CRAB_Workflow = "180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT"
== JOB AD: MaxWallTimeMins = 1250
== JOB AD: JOB_GLIDEIN_SEs = "srm.brazos.tamu.edu"
== JOB AD: CRAB_EDMOutputFiles = { "out_raw.root" }
== JOB AD: BufferBlockSize = 32768
== JOB AD: MemoryProvisioned = 2500
== JOB AD: TransferInputSizeMB = 8
== JOB AD: CRAB_DBSURL = "https://cmsweb.cern.ch/dbs/prod/phys03/DBSReader"
== JOB AD: TransferSocket = "<188.184.88.236:4080?addrs=188.184.88.236-4080&noUDP&sock=3215387_c4da_668355>"
== JOB AD: JOB_GLIDEIN_SiteWMS_Queue = "Unknown"
== JOB AD: StreamErr = false
== JOB AD: MyAddress = "<188.184.88.236:4080?addrs=188.184.88.236-4080&noUDP&sock=3215387_c4da_668355>"
== JOB AD: PeriodicRemoveReason = ifThenElse(time() - EnteredCurrentStatus > 7 * 24 * 60 * 60 && isUndefined(MemoryUsage),"Removed due to idle time limit",ifThenElse(time() > x509UserProxyExpiration,"Removed job due to proxy expiration",ifThenElse(MemoryUsage > RequestMemory,"Removed due to memory use",ifThenElse(MaxWallTimeMins * 60 < time() - EnteredCurrentStatus,"Removed due to wall clock limit",ifThenElse(DiskUsage > 20000000,"Removed due to disk usage",ifThenElse(time() > CRAB_TaskEndTime,"Removed due to reached CRAB_TaskEndTime","Removed due to job being held"))))))
== JOB AD: OVERFLOW_CHECK = ifthenelse(MATCH_GLIDEIN_CMSSite =!= undefined,ifthenelse(stringListMember(MATCH_GLIDEIN_CMSSite,DESIRED_Sites),false,true),false)
== JOB AD: CommittedTime = 0
== JOB AD: RequestDisk = 100000
== JOB AD: AcctGroup = "analysis"
== JOB AD: LocalUserCpu = 0.0
== JOB AD: CRAB_localOutputFiles = "out_raw.root=out_raw_40.root"
== JOB AD: NumJobMatches = 1
== JOB AD: CRAB_JobArch = "slc6_amd64_gcc530"
== JOB AD: MachineAttrCpus0 = 1
== JOB AD: DAGManJobId = 22981158
== JOB AD: DAGManNodesMask = "0,1,2,4,5,7,9,10,11,12,13,16,17,24,27"
== JOB AD: MachineAttrSlotWeight0 = 1
== JOB AD: RemoteUserCpu = 0.0
== JOB AD: LastJobStatus = 1
== JOB AD: UserLog = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0/job_log"
== JOB AD: ImageSize = 10
== JOB AD: JOB_CMSSite = "T3_US_TAMU"
== JOB AD: DESIRED_SITES = "T3_US_TAMU"
== JOB AD: OVERFLOW_UK = ifthenelse(regexp("T2_UK_London_",DESIRED_Sites),"True",undefined)
== JOB AD: TaskType = "Job"
== JOB AD: Iwd = "/tmp/SLURM_21303153_1507/glide_clU8RS/execute/dir_6236"
== JOB AD: CRAB_ASOURL = "https://cmsweb.cern.ch/crabserver/prod"
== JOB AD: ImageSize_RAW = 9
== JOB AD: DAGManNodesLog = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0/RunJobs.dag.nodes.log"
== JOB AD: StreamOut = false
== JOB AD: JobUniverse = 5
== JOB AD: OVERFLOW_US = ifthenelse(regexp("T[1,2]_US_",DESIRED_Sites),"True",undefined)
== JOB AD: QDate = 1525801111
== JOB AD: SpoolOnEvict = false
== JOB AD: EnteredCurrentStatus = 1525801122
== JOB AD: CRAB_ReqName = "180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT"
== JOB AD: DESIRED_CMSDataLocations = "T3_US_TAMU"
== JOB AD: CRAB_SiteWhitelist = {  }
== JOB AD: x509UserProxyExpiration = 1526391181
== JOB AD: NumShadowStarts = 1
== JOB AD: CommittedSuspensionTime = 0
== JOB AD: LastMatchTime = 1525801122
== JOB AD: PreJobPrio1 = 1
== JOB AD: JOB_GLIDEIN_SiteWMS_JobId = "21303153"
== JOB AD: JobNotification = 0
== JOB AD: x509userproxy = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0/bbe13310a0248c324d085fc8e436503c5d32cfda"
== JOB AD: ServerTime = 1525801122
== JOB AD: CRAB_TFileOutputFiles = {  }
== JOB AD: JobBatchName = "RunJobs.dag+22981158"
== JOB AD: CumulativeSlotTime = 0
== JOB AD: CRAB_TransferOutputs = 1
== JOB AD: RemoteSysCpu = 0.0
== JOB AD: CRAB_SubmitterIpAddr = "165.91.55.39"
== JOB AD: JOB_GLIDEIN_CMSSite = "T3_US_TAMU"
== JOB AD: CondorPlatform = "$CondorPlatform: x86_64_RedHat6 $"
== JOB AD: OnExitRemove = true
== JOB AD: Rank = 0.0
== JOB AD: x509UserProxyFQAN = "/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick,/cms/Role=NULL/Capability=NULL,/cms/becms/Role=NULL/Capability=NULL,/cms/uscms/Role=NULL/Capability=NULL"
== JOB AD: RemoteWallClockTime = 0.0
== JOB AD: CRAB_Id = "40"
== JOB AD: JOB_GLIDEIN_ToRetire = "1526027872"
== JOB AD: Cmd = "/data/srv/glidecondor/condor_local/spool/1158/0/cluster22981158.proc0.subproc0/gWMS-CMSRunAnalysis.sh"
== JOB AD: MachineAttrHAS_SINGULARITY0 = true
== JOB AD: JOB_Gatekeeper = ifthenelse(substr(Used_Gatekeeper,0,1) =!= "$",Used_Gatekeeper,ifthenelse(MATCH_GLIDEIN_Gatekeeper =!= undefined,MATCH_GLIDEIN_Gatekeeper,"Unknown"))
== JOB AD: AccountingGroup = "analysis.dildick"
== JOB AD: JobCurrentStartDate = 1525801122
== JOB AD: use_x509userproxy = true
== JOB AD: DiskUsage = 10000
== JOB AD: CRAB_Retry = 5
== JOB AD: CRAB_Destination = "srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/log/cmsRun_40.log.tar.gz, srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/out_raw_40.root"
== JOB AD: ClusterId = 22994598
== JOB AD: CRAB_StageoutPolicy = "local,remote"
== JOB AD: RequestCpus = 1
== JOB AD: CondorVersion = "$CondorVersion: 8.6.8 Oct 30 2017 BuildID: 422919 $"
== JOB AD: CRAB_TaskWorker = "vocms052"
== JOB AD: JOB_GLIDEIN_Job_Max_Time = "14400"
== JOB AD: accounting_group = analysis
==== HTCONDOR JOB AD CONTENTS FINISH ====
======== HTCONDOR JOB SUMMARY at Tue May  8 17:38:52 GMT 2018 FINISH ========
======== PROXY INFORMATION START at Tue May  8 17:38:52 GMT 2018 ========
subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick/CN=987802512/CN=2087825196/CN=1899266192/CN=18534653/CN=656397437/CN=724400330
issuer    : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick/CN=987802512/CN=2087825196/CN=1899266192/CN=18534653/CN=656397437
identity  : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick/CN=987802512/CN=2087825196/CN=1899266192/CN=18534653/CN=656397437
type      : RFC compliant proxy
strength  : 1024 bits
path      : /srv/bbe13310a0248c324d085fc8e436503c5d32cfda
timeleft  : 23:59:53
key usage : Digital Signature, Key Encipherment
=== VO cms extension information ===
VO        : cms
subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dildick/CN=710593/CN=Sven Dildick
issuer    : /DC=ch/DC=cern/OU=computers/CN=lcg-voms2.cern.ch
attribute : /cms/Role=NULL/Capability=NULL
attribute : /cms/becms/Role=NULL/Capability=NULL
attribute : /cms/uscms/Role=NULL/Capability=NULL
timeleft  : 163:54:10
uri       : lcg-voms2.cern.ch:15002
======== PROXY INFORMATION FINISH at Tue May  8 17:38:52 GMT 2018 ========
======== CMSRunAnalysis.sh at Tue May  8 17:38:52 GMT 2018 STARTING ========
======== CMSRunAnalysis.sh STARTING at Tue May  8 17:38:52 GMT 2018 ========
Local time : Tue May  8 17:38:52 UTC 2018
Current system : Linux c0112.brazos.tamu.edu 4.4.128-1.el6.elrepo.x86_64 #1 SMP Sat Apr 14 08:41:22 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
==== CMSSW pre-execution environment bootstrap STARTING ====
+ '[' -f /cmsset_default.sh ']'
+ '[' -f /home/osg/app/cmssoft/cms/cmsset_default.sh ']'
+ '[' -f /cvmfs/cms.cern.ch/cmsset_default.sh ']'
+ echo 'CVMFS style'
CVMFS style
+ export VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ set +x
+ declare -a VERSIONS
+ VERSIONS=($(ls /cvmfs/cms.cern.ch/$SCRAM_ARCH/external/python | egrep '2.[67]'))
++ egrep '2.[67]'
++ ls /cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python
+ PY_PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python
+ echo 'python version: ' 2.7.11-giojec
python version:  2.7.11-giojec
+ set +x
==== CMSSW pre-execution environment bootstrap FINISHING at Tue May  8 17:38:52 GMT 2018 ====
==== Python discovery STARTING ====
Python found in /cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec
I found python at..
/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec/bin/python
==== Python discovery FINISHING at Tue May  8 17:38:52 GMT 2018 ====
======== Current environment dump STARTING ========
== ENV: OSG_SINGULARITY_VERSION=2.5.0-dist
== ENV: LCMAPS_DB_FILE=/etc/lcmaps.db
== ENV: GLIDEIN_ResourceName=TAMU_BRAZOS_CE
== ENV: GLIDEIN_SEs=srm.brazos.tamu.edu
== ENV: SLURM_CHECKPOINT_IMAGE_DIR=/var/lib/slurm/checkpoint
== ENV: SLURM_NODELIST=c0112
== ENV: _CONDOR_JOB_PIDS=
== ENV: MANPATH=/cvmfs/cms.cern.ch/share/man:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/man:/apps/lmod/lmod/share/man::
== ENV: CMS_GLIDEIN_VERSION=10
== ENV: SLURM_JOB_NAME=GRIDJOB
== ENV: GRIDMAP=/tmp/SLURM_21303153_1507/glide_clU8RS/grid-mapfile
== ENV: SLURMD_NODENAME=c0112
== ENV: GLIDEIN_REQUIRE_GLEXEC_USE=False
== ENV: HOSTNAME=c0112.brazos.tamu.edu
== ENV: SLURM_TOPOLOGY_ADDR=c0112
== ENV: OSG_GLEXEC_LOCATION=/usr/local/bin/glexec
== ENV: REQUIRED_OS=rhel6
== ENV: SLURM_NODE_ALIASES=(null)
== ENV: OSG_SINGULARITY_BIND_GPU_LIBS=0
== ENV: GFAL_PLUGIN_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/gfal2-plugins/
== ENV: TERM=xterm
== ENV: VOMS_USERCONF=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/vomses
== ENV: LCMAPS_POLICY_NAME=authorize_only
== ENV: HISTSIZE=1000
== ENV: SLURM_JOB_QOS=hepx
== ENV: qos
== ENV: OSG_MACHINE_GPUS=0
== ENV: SLURM_TOPOLOGY_ADDR_PATTERN=node
== ENV: GLOBUS_LOCATION=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr
== ENV: LMOD_DEFAULT_MODULEPATH=/apps/modulefiles/Linux:/apps/modulefiles/Core:/apps/lmod/lmod/modulefiles/Core
== ENV: LMOD_SYSTEM_DEFAULT_MODULES=brazos
== ENV: TMPDIR=/srv
== ENV: MODULEPATH_ROOT=/apps/modulefiles
== ENV: PERL5LIB=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/perl5/vendor_perl:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/share/perl5:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/perl5/vendor_perl:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/perl5
== ENV: GLIDECLIENT_Signature=2148ef66640a2d6965b50dd6511f716b5a0a879b
== ENV: GAHP_TEMP=/tmp/condor_g_scratch.0x7f483cfaf450.1143867
== ENV: LMOD_PACKAGE_PATH=/apps/modulefiles/Site
== ENV: CVSROOT=:gserver:cmssw.cvs.cern.ch:/local/reps/CMSSW
== ENV: LMOD_PKG=/apps/lmod/6.0.15
== ENV: _CONDOR_SCRATCH_DIR=/srv
== ENV: HAS_CVMFS_singularity_opensciencegrid_org=True
== ENV: QTDIR=/usr/lib64/qt-3.3
== ENV: X509_CERT_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/grid-security/certificates
== ENV: CMS_VALIDATION_FRONTIER=0
== ENV: SCRAM_ARCH=slc6_amd64_gcc530
== ENV: LMOD_ADMIN_FILE=/apps/modulefiles/Site/admin.list
== ENV: OSG_WN_TMP=/tmp
== ENV: QTINC=/usr/lib64/qt-3.3/include
== ENV: LMOD_VERSION=6.0.15
== ENV: GLIDEIN_Max_Idle=600
== ENV: SCHEDD_NAME=ce01.brazos.tamu.edu
== ENV: _CONDOR_ANCESTOR_17207=6236:1525801123:1979906417
== ENV: OSG_SQUID_LOCATION=squid.brazos.tamu.edu:3128
== ENV: GLIDEIN_REQUIRE_VOMS=False
== ENV: GLEXEC_BIN=NONE
== ENV: NO_PROXY=.brazos.tamu.edu
== ENV: NODE_COUNT=1
== ENV: JOBSTARTDIR=/srv
== ENV: SLURM_NNODES=1
== ENV: http_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: USER=dildick
== ENV: _CHIRP_DELAYED_UPDATE_PREFIX=Chirp*
== ENV: OSG_SITE_WRITE=/fdata/scratch/osg
== ENV: CRAB_Retry=5
== ENV: LD_LIBRARY_PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/sqlite/3.12.2/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/libffi/3.2.1/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/gdbm/1.10/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/readline/6.3/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/ncurses/6.0-20151128-ikhhed/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/db6/6.0.30/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/expat/2.1.0/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/openssl/1.0.2d/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/bz2lib/1.0.6/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/zlib/1.2.8/lib:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/gcc/5.3.0/lib64:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/gcc/5.3.0/lib:/srv/condor/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/lib64:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/dcap:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/lcgdm
== ENV: LMOD_sys=Linux
== ENV: CONDOR_PARENT_ID=c0112:6236:1525801124
== ENV: CMSSITE=T3_US_TAMU
== ENV: GLIDEIN_Entry_Name=CMS_T3_TAMU_BRAZOS_ce01
== ENV: OSG_SINGULARITY_IMAGE=/cvmfs/singularity.opensciencegrid.org/bbockelm/cms:rhel6
== ENV: VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
== ENV: GLIDEIN_Name=v3_2
== ENV: BATCH_SYSTEM=HTCondor
== ENV: OSG_LOCATION=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64
== ENV: SINGULARITY_NAME=86b5301ef4edf5ab73a46c6b412b31d1894ec1823cbc995afeb2a68e1faf8d
== ENV: ENV=/etc/profile
== ENV: OSG_SINGULARITY_IMAGE_DEFAULT=/cvmfs/singularity.opensciencegrid.org/bbockelm/cms:rhel6
== ENV: LLGT_LIFT_PRIVILEGED_PROTECTION=1
== ENV: CVMFS_oasis_opensciencegrid_org_TIMESTAMP=1525198991
== ENV: _CONDOR_CHIRP_CONFIG=/srv/.chirp.config
== ENV: SLURM_JOBID=21303153
== ENV: GLOBUS_GSSAPI_MAX_TLS_PROTOCOL=0
== ENV: FTP_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: ftp_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: GLIDEIN_Entry_Signature=d97277c2c7730292695037a14c007c24a6030d34
== ENV: CONDORCE_COLLECTOR_HOST=ce01.brazos.tamu.edu:9619
== ENV: OSG_SITE_READ=/fdata/scratch/osg
== ENV: LLGT_LOG_IDENT=htcondor-ce
== ENV: CONDOR_PROCD_ADDRESS_BASE=/tmp/SLURM_21303153_1507/glide_clU8RS/log/procd_address
== ENV: _CONDOR_ANCESTOR_15527=17207:1525800303:1421820823
== ENV: LMOD_PREPEND_BLOCK=normal
== ENV: GLIDEIN_Site=TAMU_BRAZOS
== ENV: GLIDECLIENT_Name=CMSG-v1_0.local_users
== ENV: OSG_SINGULARITY_REEXEC=1
== ENV: PATH=/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/python/2.7.11-giojec/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/sqlite/3.12.2/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/gdbm/1.10/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/ncurses/6.0-20151128-ikhhed/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/db6/6.0.30/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/expat/2.1.0/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/openssl/1.0.2d/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/bz2lib/1.0.6/bin:/cvmfs/cms.cern.ch/slc6_amd64_gcc530/external/gcc/5.3.0/bin:/cvmfs/cms.cern.ch/common:/cvmfs/cms.cern.ch/bin:/srv/condor/libexec:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/bin:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/bin:/usr/local/sbin
== ENV: MAIL=/var/spool/mail/dildick
== ENV: SLURM_TASKS_PER_NODE=1
== ENV: _ModuleTable001_=X01vZHVsZVRhYmxlXz17WyJhY3RpdmVTaXplIl09MSxiYXNlTXBhdGhBPXsiL2FwcHMvbW9kdWxlZmlsZXMvTGludXgiLCIvYXBwcy9tb2R1bGVmaWxlcy9Db3JlIiwiL2FwcHMvbG1vZC9sbW9kL21vZHVsZWZpbGVzL0NvcmUiLH0sWyJjX3JlYnVpbGRUaW1lIl09ZmFsc2UsWyJjX3Nob3J0VGltZSJdPWZhbHNlLGZhbWlseT17fSxpbmFjdGl2ZT17fSxtVD17YnJhem9zPXtbIkZOIl09Ii9hcHBzL21vZHVsZWZpbGVzL0NvcmUvYnJhem9zLmx1YSIsWyJkZWZhdWx0Il09MCxbImZ1bGxOYW1lIl09ImJyYXpvcyIsWyJsb2FkT3JkZXIiXT0xLHByb3BUPXtsbW9kPXtbInN0aWNreSJdPTEsfSx9LFsic2hvcnQiXT0iYnJhem9zIixbInN0YXR1cyJdPSJhY3RpdmUiLH0sfSxtcGF0
== ENV: LCMAPS_DEBUG_LEVEL=2
== ENV: OSG_SINGULARITY_OUTSIDE_PWD=/tmp/SLURM_21303153_1507/glide_clU8RS/execute/dir_6236
== ENV: SLURM_CONF=/etc/slurm/slurm.conf
== ENV: HAS_CVMFS_oasis_opensciencegrid_org=True
== ENV: LCMAPS_DIR=/etc
== ENV: HOLD_GRACE_TIME=0
== ENV: CMS_PATH=/cvmfs/cms.cern.ch
== ENV: SLURM_JOB_ID=21303153
== ENV: CONDOR_CONFIG=/tmp/SLURM_21303153_1507/glide_clU8RS/condor_config
== ENV: OSG_DATA=/fdata/scratch/osg
== ENV: LMOD_SETTARG_CMD=:
== ENV: SLURM_JOB_USER=dildick
== ENV: OSG_APP=/home/osg/app
== ENV: PWD=/srv
== ENV: _LMFILES_=/apps/modulefiles/Core/brazos.lua
== ENV: CRAB_Id=40
== ENV: LMOD_PAGER=more
== ENV: LANG=C
== ENV: _CONDOR_WRAPPER_ERROR_FILE=/tmp/SLURM_21303153_1507/glide_clU8RS/execute/dir_6236/.job_wrapper_failure
== ENV: GLIDEIN_Country=US
== ENV: MODULEPATH=/apps/modulefiles/Linux:/apps/modulefiles/Core:/apps/lmod/lmod/modulefiles/Core
== ENV: TZ=UTC
== ENV: X509_VOMS_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/grid-security/vomsdir
== ENV: LOADEDMODULES=brazos
== ENV: _ModuleTable_Sz_=2
== ENV: GLIDEIN_Gatekeeper=ce01.brazos.tamu.edu
== ENV: ce01.brazos.tamu.edu:9619
== ENV: SLURM_JOB_UID=1507
== ENV: GATEKEEPER_JM_ID=2018-05-08.17:09:23.0001143867.0000000985
== ENV: GLIDEIN_Job_Max_Time=14400
== ENV: SLURM_NODEID=0
== ENV: _CONDOR_SLOT=
== ENV: OSG_GRID=/etc/osg/wn-client/
== ENV: CRAB_TASKMANAGER_TARBALL=local
== ENV: GLIDEIN_Tmp_Dir=/tmp/SLURM_21303153_1507/glide_clU8RS/tmp
== ENV: SLURM_SUBMIT_DIR=/work/condor/9587/0/cluster2459587.proc0.subproc0
== ENV: GLOBUS_TCP_PORT_RANGE_STATE_FILE=
== ENV: GLIDEIN_Proxy_URL=None
== ENV: SLURM_TASK_PID=24579
== ENV: GLIDEIN_CLAIM_WORKLIFE_DYNAMIC=cpus*(6*3600)
== ENV: LMOD_CMD=/apps/lmod/6.0.15/libexec/lmod
== ENV: GLIDEIN_Factory=CMS-CERN-Production
== ENV: GLIDEIN_GridType=condor
== ENV: OSG_SINGULARITY_PATH=/usr/bin/singularity
== ENV: https_proxy=http://squid.brazos.tamu.edu:3128/
== ENV: HTTPS_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: GLIDECLIENT_Group=local_users
== ENV: LMOD_AVAIL_STYLE=grouped:system
== ENV: SLURM_CPUS_ON_NODE=1
== ENV: OSG_HOSTNAME=ce01.brazos.tamu.edu
== ENV: SLURM_PROCID=0
== ENV: HISTCONTROL=ignoredups
== ENV: ENVIRONMENT=BATCH
== ENV: _CONDOR_ANCESTOR_6236=6370:1525801131:339447166
== ENV: SLURM_JOB_NODELIST=c0112
== ENV: _CONDOR_EXECUTE=/tmp/SLURM_21303153_1507/glide_clU8RS/execute
== ENV: GLIDEIN_Expose_Grid_Env=True
== ENV: GLIDECLIENT_Group_Signature=43f900294971967f4e826d823e02a377e7d8af62
== ENV: SHLVL=8
== ENV: HAS_CVMFS_cms_cern_ch=True
== ENV: GLOBUS_GSSAPI_NAME_COMPATIBILITY=STRICT_RFC2818
== ENV: HOME=/srv
== ENV: _CONDOR_MACHINE_AD=/srv/.machine.ad
== ENV: GLOBUS_TCP_PORT_RANGE=40000,41999
== ENV: OSG_STORAGE_ELEMENT=True
== ENV: GLIDEIN_Max_Tail=600
== ENV: SLURM_LOCALID=0
== ENV: X509_USER_PROXY=/srv/bbe13310a0248c324d085fc8e436503c5d32cfda
== ENV: GSI_AUTHZ_CONF=/dev/null
== ENV: java_options=-Xms128m
== ENV: -Xmx512m
== ENV: _ModuleTable002_=aEE9eyIvYXBwcy9tb2R1bGVmaWxlcy9MaW51eCIsIi9hcHBzL21vZHVsZWZpbGVzL0NvcmUiLCIvYXBwcy9sbW9kL2xtb2QvbW9kdWxlZmlsZXMvQ29yZSIsfSxbInN5c3RlbUJhc2VNUEFUSCJdPSIvYXBwcy9tb2R1bGVmaWxlcy9MaW51eDovYXBwcy9tb2R1bGVmaWxlcy9Db3JlOi9hcHBzL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIixbInZlcnNpb24iXT0yLH0=
== ENV: no_proxy=.brazos.tamu.edu
== ENV: SLURM_CLUSTER_NAME=brazos
== ENV: SLURM_JOB_GID=2015
== ENV: JOB_REPOSITORY_ID=2018-05-08.17:09:23.0001143867.0000000985
== ENV: GLOBUS_GSSAPI_MIN_TLS_PROTOCOL=TLS1_VERSION
== ENV: SLURM_JOB_CPUS_PER_NODE=1
== ENV: SLOTS_LAYOUT=fixed
== ENV: GFAL_CONFIG_DIR=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/etc/gfal2.d/
== ENV: GLIDEIN_CMSSite=T3_US_TAMU
== ENV: SLURM_SUBMIT_HOST=ce01.brazos.tamu.edu
== ENV: HTTP_PROXY=http://squid.brazos.tamu.edu:3128/
== ENV: CVMFS_oasis_opensciencegrid_org_REVISION=9100
== ENV: GLIDEIN_Signature=b5a1e623f9556c0898d245397da0d07f2b8b5d10
== ENV: SLURM_GTIDS=0
== ENV: GLIDEIN_Schedd=schedd_glideins5@vocms0805.cern.ch
== ENV: SLURM_JOB_PARTITION=stakeholder
== ENV: OSG_DEFAULT_SE=srm.brazos.tamu.edu
== ENV: PYTHONPATH=/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
== ENV: LMOD_arch=x86_64
== ENV: LOGNAME=dildick
== ENV: GLIDEIN_Glexec_Use=OPTIONAL
== ENV: LESS=-XqeMRF
== ENV: GLIDEIN_REQUIRED_OS=any
== ENV: GPUDetection=No
== ENV: GPUs
== ENV: detected
== ENV: CVMFS_cms_cern_ch_REVISION=54193
== ENV: QTLIB=/usr/lib64/qt-3.3/lib
== ENV: HAS_SINGULARITY=1
== ENV: CVS_RSH=ssh
== ENV: OSG_SITE_NAME=TAMU_BRAZOS_CE
== ENV: SLURM_JOB_ACCOUNT=hepx
== ENV: GLIDEIN_Monitoring_Enabled=False
== ENV: MODULESHOME=/apps/lmod/6.0.15
== ENV: GLIDEIN_ClusterId=1256895
== ENV: GLIDEIN_ProcId=8
== ENV: SLURM_JOB_NUM_NODES=1
== ENV: OMP_NUM_THREADS=1
== ENV: GLOBUS_GSSAPI_CIPHERS=HIGH
== ENV: GLOBUS_GSSAPI_BACKWARD_COMPATIBLE_MIC=true
== ENV: LESSOPEN=||/usr/bin/lesspipe.sh
== ENV: %s
== ENV: __Init_Default_Modules=1
== ENV: _CONDOR_JOB_AD=/srv/.job.ad
== ENV: SINGULARITY_CONTAINER=86b5301ef4edf5ab73a46c6b412b31d1894ec1823cbc995afeb2a68e1faf8d
== ENV: LMOD_FULL_SETTARG_SUPPORT=no
== ENV: OSG_SINGULARITY_AUTOLOAD=1
== ENV: GLITE_LOCATION=/usr/libexec/condor/glite
== ENV: CVMFS_singularity_opensciencegrid_org_REVISION=34335
== ENV: CRAB_RUNTIME_TARBALL=local
== ENV: GLOBUS_GSSAPI_FORCE_TLS=false
== ENV: GLIDEIN_LOCAL_TMP_DIR=/tmp/glide_dildick_vdouSV
== ENV: LMOD_DIR=/apps/lmod/6.0.15/libexec
== ENV: _CONDOR_JOB_IWD=/srv
== ENV: OSG_SINGULARITY_BIND_CVMFS=1
== ENV: SLURM_MEM_PER_NODE=1900
== ENV: G_BROKEN_FILENAMES=1
== ENV: LMOD_COLORIZE=yes
== ENV: SCRATCH=/fdata/scratch/dildick
== ENV: USER_DN=/DC=ch/DC=cern/OU=Organic
== ENV: Units/OU=Users/CN=dildick/CN=710593/CN=Sven
== ENV: Dildick
== ENV: CONDOR_PROCD_ADDRESS=/tmp/SLURM_21303153_1507/glide_clU8RS/log/procd_address
== ENV: GLOBUS_GSSAPI_SERVER_CIPHER_ORDER=true
== ENV: _=/bin/env
======== Current environment dump FINISHING ========
======== Tarball initialization STARTING at Tue May  8 17:38:52 GMT 2018 ========
+ [[ X == \X ]]
+ [[ local == \l\o\c\a\l ]]
+ tar xzmf CMSRunAnalysis.tar.gz
++ pwd
++ pwd
+ export PYTHONPATH=/srv/CRAB3.zip:/srv/WMCore.zip:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
+ PYTHONPATH=/srv/CRAB3.zip:/srv/WMCore.zip:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib/python2.6/site-packages:/cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.4/3.4.10/el6-x86_64/usr/lib64/python2.6/site-packages
+ set +x
======== Tarball initialization FINISHING at Tue May  8 17:38:52 GMT 2018 ========
==== Local directory contents dump STARTING ====
PWD: /srv
== DIR: ApmonIf.py
== DIR: CMSGroupMapper.py
== DIR: CMSRunAnalysis.py
== DIR: CMSRunAnalysis.sh
== DIR: CMSRunAnalysis.tar.gz
== DIR: DashboardAPI.py
== DIR: DashboardFailure.sh
== DIR: Logger.py
== DIR: ProcInfo.py
== DIR: RESTInteractions.py
== DIR: ServerUtilities.py
== DIR: TweakPSet.py
== DIR: WMArchiveReport.json
== DIR: WMArchiveReport.json.40
== DIR: WMCore.zip
== DIR: _condor_stderr
== DIR: _condor_stdout
== DIR: apmon.py
== DIR: bbe13310a0248c324d085fc8e436503c5d32cfda
== DIR: cmscp.py
== DIR: condor
== DIR: condor_exec.exe
== DIR: input_files.tar.gz
== DIR: jobReport.json
== DIR: jobReport.json.40
== DIR: libcurl.so.4
== DIR: run_and_lumis.tar.gz
== DIR: sandbox.tar.gz
== DIR: startup_environment.sh
==== Local directory contents dump FINISHING ====
======== CMSRunAnalysis.py STARTING at Tue May  8 17:38:52 GMT 2018 ========
Now running the CMSRunAnalysis.py job in /srv...
++ pwd
+ python CMSRunAnalysis.py -r /srv -a sandbox.tar.gz --sourceURL=https://cmsweb.cern.ch/crabcache --jobNumber=40 --cmsswVersion=CMSSW_8_0_30 --scramArch=slc6_amd64_gcc530 --inputFile=job_input_file_list_40.txt --runAndLumis=job_lumis_40.json --lheInputFiles=False --firstEvent=None --firstLumi=None --lastEvent=None --firstRun=None --seeding=AutomaticSeeding --scriptExe=None --eventsPerLumi=None --maxRuntime=-60 '--scriptArgs=[]' -o '{}' --oneEventMode=0
==== CMSRunAnalysis.py STARTING at Tue May  8 17:38:52 2018 ====
Local time : Tue May  8 17:38:52 2018
Dashboard early startup params: {'MonitorID': '180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT', 'MonitorJobID': '40_https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT_5', 'SyncCE': 'ce01.brazos.tamu.edu', 'OverflowFlag': 0, 'SyncSite': 'T3_US_TAMU', 'SyncGridJobId': 'https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT', 'WNHostName': 'c0112.brazos.tamu.edu'}
==== Parameters Dump at Tue May  8 17:38:52 2018 ===
archiveJob:     sandbox.tar.gz
runDir:         /srv
sourceURL:      https://cmsweb.cern.ch/crabcache
jobNumber:      40
cmsswVersion:   CMSSW_8_0_30
scramArch:      slc6_amd64_gcc530
inputFile       job_input_file_list_40.txt
outFiles:       {}
runAndLumis:    job_lumis_40.json
lheInputFiles:  False
firstEvent:     None
firstLumi:      None
eventsPerLumi:  None
lastEvent:      None
firstRun:       None
seeding:        AutomaticSeeding
userFiles:      None
oneEventMode:   0
scriptExe:      None
scriptArgs:     []
maxRuntime:     -60
===================
==== Sandbox preparation STARTING at Tue May  8 17:38:52 2018 ====
Sandbox sandbox.tar.gz already exists, skipping

==== Sandbox preparation FINISHING at Tue May  8 17:38:53 2018 ====
==== WMCore filesystem preparation STARTING at Tue May  8 17:38:53 2018 ====
==== WMCore filesystem preparation FINISHING at Tue May  8 17:38:53 2018 ====
Dashboard startup parameters: {'MonitorID': '180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT', 'MonitorJobID': '40_https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT_5', 'WNHostName': 'c0112.brazos.tamu.edu', 'ExeStart': 'cmsRun'}
==== CMSSW Stack Execution STARTING at Tue May  8 17:38:53 2018 ====
==== SCRAM Obj CREATED at Tue May  8 17:38:53 2018 ====

==== CMSSW JOB Execution started at Tue May  8 17:38:55 2018 ====
2018-05-08 17:38:55,524:INFO:Scram:    Invoking command: python -c 'from PSetTweaks.WMTweak import makeTweak;config = __import__("WMTaskSpace.cmsRun.PSet", globals(), locals(), ["process"], -1);tweakJson = makeTweak(config.process).jsondictionary();print tweakJson["process"]["outputModules_"]'
2018-05-08 17:39:03,265:INFO:CMSSW:User files are 
2018-05-08 17:39:03,265:INFO:CMSSW:User sandboxes are sandbox.tar.gz
2018-05-08 17:39:03,265:INFO:CMSSW:CMSSW configured for 1 cores and 0 event streams
2018-05-08 17:39:03,265:INFO:CMSSW:Executing CMSSW step
2018-05-08 17:39:03,265:INFO:CMSSW:Runing SCRAM
2018-05-08 17:39:04,070:INFO:CMSSW:Running PRE scripts
2018-05-08 17:39:04,071:INFO:CMSSW:RUNNING SCRAM SCRIPTS
2018-05-08 17:39:04,077:INFO:Scram:    Invoking command: python /srv/TweakPSet.py --location=/srv --inputFile='job_input_file_list_40.txt' --runAndLumis='job_lumis_40.json' --firstEvent=None --lastEvent=None --firstLumi=None --firstRun=None --seeding=AutomaticSeeding --lheInputFiles=False --oneEventMode=0 --eventsPerLumi=None --maxRuntime=-60 

2018-05-08 17:39:20,235:INFO:CMSSW:Executing CMSSW. args: ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc530', 'scramv1', 'CMSSW', 'CMSSW_8_0_30', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']
2018-05-08 17:40:10,813:CRITICAL:CMSSW:Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc530', 'scramv1', 'CMSSW', 'CMSSW_8_0_30', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

==== CMSSW Stack Execution FAILED at Tue May  8 17:40:10 2018 ====
======== CMSSW OUTPUT STARTING ========
NOTICE: lines longer than 3000 characters will be truncated
== CMSSW: Beginning CMSSW wrapper script
== CMSSW:  slc6_amd64_gcc530 scramv1 CMSSW
== CMSSW: Performing SCRAM setup...
== CMSSW: Completed SCRAM setup
== CMSSW: Retrieving SCRAM project...
== CMSSW: Untarring  /srv/sandbox.tar.gz
== CMSSW: Completed SCRAM project
== CMSSW: Executing CMSSW
== CMSSW: cmsRun  -j FrameworkJobReport.xml PSet.py
== CMSSW: 08-May-2018 17:40:07 UTC  Initiating request to open file /fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root
== CMSSW: 08-May-2018 17:40:07 UTC  Fallback request to file root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root
== CMSSW: %MSG-w XrdAdaptorInternal:  file_open 08-May-2018 17:40:10 UTC pre-events
== CMSSW: Failed to open file at URL root://cms-xrd-global.cern.ch:1094//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root?tried=+1213xrootd.ba.infn.it.
== CMSSW: %MSG
== CMSSW: %MSG-w XrdAdaptorInternal:  file_open 08-May-2018 17:40:10 UTC pre-events
== CMSSW: Failed to open file at URL root://cms-xrd-global.cern.ch:1094//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root?tried=+1213xrootd.ba.infn.it,.
== CMSSW: %MSG
== CMSSW: ----- Begin Fatal Exception 08-May-2018 17:40:10 UTC-----------------------
== CMSSW: An exception of category 'FallbackFileOpenError' occurred while
== CMSSW:    [0] Constructing the EventProcessor
== CMSSW:    [1] Constructing input source of type PoolSource
== CMSSW:    [2] Calling RootFileSequenceBase::initTheFile()
== CMSSW:    [3] Calling StorageFactory::open()
== CMSSW:    [4] Calling XrdFile::open()
== CMSSW: Exception Message:
== CMSSW: Failed to open the file 'root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root'
== CMSSW:    Additional Info:
== CMSSW:       [a] open() failed with system error 'No such file or directory' (error code 2)
== CMSSW:       [b] Input file /fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root could not be opened.
== CMSSW: Fallback Input file root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root also could not be opened.
== CMSSW: Original exception info is above; fallback exception info is below.
== CMSSW:       [c] XrdCl::File::Open(name='root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root', flags=0x10, permissions=0660) => error '[ERROR] Server responded with an error: [3011] No servers are available to read the file.
== CMSSW: ' (errno=3011, code=400). No additional data servers were found.
== CMSSW:       [d] Last URL tried: root://cms-xrd-global.cern.ch:1094//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root?tried=+1213xrootd.ba.infn.it,
== CMSSW:       [e] Problematic data server: cms-xrd-global.cern.ch:1094
== CMSSW:       [f] Disabled source: cms-xrd-global.cern.ch:1094
== CMSSW: ----- End Fatal Exception -------------------------------------------------
== CMSSW: Complete
== CMSSW: process id is 269 status is 92
======== CMSSW OUTPUT FINSHING ========
ERROR: Caught WMExecutionFailure - code = 92 - name = CmsRunFailure - detail = Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc530', 'scramv1', 'CMSSW', 'CMSSW_8_0_30', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

NOTE: FJR has exit code 8028 and WMCore reports 92; preferring the FJR one.
ERROR: Exceptional exit at Tue May  8 17:40:10 2018 (8028): CmsRunFailure
CMSSW error message follows.
Fatal Exception
An exception of category 'FallbackFileOpenError' occurred while
   [0] Constructing the EventProcessor
   [1] Constructing input source of type PoolSource
   [2] Calling RootFileSequenceBase::initTheFile()
   [3] Calling StorageFactory::open()
   [4] Calling XrdFile::open()
Exception Message:
Failed to open the file 'root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root'
   Additional Info:
      [a] open() failed with system error 'No such file or directory' (error code 2)
      [b] Input file /fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root could not be opened.
Fallback Input file root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root also could not be opened.
Original exception info is above; fallback exception info is below.
      [c] XrdCl::File::Open(name='root://xrootd.ba.infn.it//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root', flags=0x10, permissions=0660) => error '[ERROR] Server responded with an error: [3011] No servers are available to read the file.
' (errno=3011, code=400). No additional data servers were found.
      [d] Last URL tried: root://cms-xrd-global.cern.ch:1094//store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/DarkSUSY_mH_125_mGammaD_0250_13TeV/180508_051351/0000/Pythia8HadronizerFilter_13TeV_cfi_py_GEN_SIM_134.root?tried=+1213xrootd.ba.infn.it,
      [e] Problematic data server: cms-xrd-global.cern.ch:1094
      [f] Disabled source: cms-xrd-global.cern.ch:1094

ERROR: Traceback follows:
 Traceback (most recent call last):
  File "CMSRunAnalysis.py", line 903, in <module>
    cmssw = executeCMSSWStack(opts, scram)
  File "CMSRunAnalysis.py", line 701, in executeCMSSWStack
    cmssw.execute()
  File "/srv/WMCore.zip/WMCore/WMSpec/Steps/Executors/CMSSW.py", line 251, in execute
    raise WMExecutionFailure(returncode, "CmsRunFailure", msg)
WMExecutionFailure: CmsRunFailure
Message: Error running cmsRun
{'arguments': ['/bin/bash', '/srv/cmsRun-main.sh', '', 'slc6_amd64_gcc530', 'scramv1', 'CMSSW', 'CMSSW_8_0_30', 'FrameworkJobReport.xml', 'cmsRun', 'PSet.py', 'sandbox.tar.gz', '', '']}
Return code: 92

	ModuleName : WMCore.WMSpec.Steps.WMExecutionFailure
	MethodName : __init__
	ClassInstance : None
	FileName : /srv/WMCore.zip/WMCore/WMSpec/Steps/WMExecutionFailure.py
	ClassName : None
	LineNumber : 18
	ErrorNr : 92

Traceback: 



== Execution site for failed job from site-local-config.xml: T3_US_TAMU
Dashboard end parameters: {'MonitorID': '180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT', 'MonitorJobID': '40_https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT_5', 'NEventsProcessed': 0, 'JobExitCode': 8028, 'NCores': 1, 'ExeExitCode': 8028}
Not sending data to popularity service because no input sources found.
Dashboard popularity report: {'Basename': '', 'inputFiles': '', 'BasenameParent': '', 'inputBlocks': '/DarkSUSY_mH_125_mGammaD_0250_13TeV/dildick-DarkSUSY_mH_125_mGammaD_0250_13TeV-56b8d97cfc2d79b4eb9b54680fe363d1/USER#cf9a8571-274d-4a22-b465-a7db5e1cd5ae', 'parentFiles': ''}
+ jobrc=92
+ set +x
== The job had an exit code of 92 
======== CMSRunAnalysis.py FINISHING at Tue May  8 17:40:11 GMT 2018 ========
==== SCRAM interaction log contents dump STARTING ====
Log for recording SCRAM command-line output
-------------------------------------------
Beginning TweakPSet
 arguments: ['/srv/TweakPSet.py', '--location=/srv', '--inputFile=job_input_file_list_40.txt', '--runAndLumis=job_lumis_40.json', '--firstEvent=None', '--lastEvent=None', '--firstLumi=None', '--firstRun=None', '--seeding=AutomaticSeeding', '--lheInputFiles=False', '--oneEventMode=0', '--eventsPerLumi=None', '--maxRuntime=-60']
One event mode disabled until we can put together a decent version of WMCore.
TweakPSet.py is going to force one event mode
Tag chirp updates from CMSSW with _cmsRun_
==== SCRAM interaction log contents dump FINISHING ====

real	1m19.391s
user	1m0.776s
sys	0m7.222s
CMSRunAnalysis.sh complete at Tue May  8 17:40:11 GMT 2018 with (short) exit status 92
======== CMSRunAnalsysis.sh at Tue May  8 17:40:11 GMT 2018 FINISHING ========
======== python2.6 bootstrap for stageout at Tue May  8 17:40:11 GMT 2018 STARTING ========
+ '[' -f /cmsset_default.sh ']'
+ '[' -f /home/osg/app/cmssoft/cms/cmsset_default.sh ']'
+ '[' -f /cvmfs/cms.cern.ch/cmsset_default.sh ']'
+ export VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ VO_CMS_SW_DIR=/cvmfs/cms.cern.ch
+ export CMS_PATH=/cvmfs/cms.cern.ch
+ CMS_PATH=/cvmfs/cms.cern.ch
+ set +x
+ '[' -e /cvmfs/cms.cern.ch/COMP/slc6_amd64_gcc493/external/python/2.7.6/etc/profile.d/init.sh ']'
+ set +x
+ command -v python2.7
+ rc=0
+ set +x
Found python2.7 at:
/cvmfs/cms.cern.ch/COMP/slc6_amd64_gcc493/external/python/2.7.6/bin/python2.7
======== python2.7 bootstrap for stageout at Tue May  8 17:40:11 GMT 2018 FINISHING ========
======== Attempting to notify HTCondor of file stageout ========
	Error: 22 (Invalid argument)
======== Stageout at Tue May  8 17:40:11 GMT 2018 STARTING ========
====== Tue May  8 17:40:12 2018: cmscp.py STARTING.
Stageout policy: local, remote
====== Tue May  8 17:40:12 2018: Starting job report validation.
Job report seems ok (it has the expected structure).
Retrieved payload exit code ('jobExitCode') = 8028 from job report.
Retrieved job wrapper exit code ('exitCode') = 8028 from job report.
====== Tue May  8 17:40:12 2018: Finished job report validation (status 0).
====== Tue May  8 17:40:12 2018: Starting to check if user output files exist.
ERROR: Output file out_raw.root does not exist.
====== Tue May  8 17:40:12 2018: Finished to check if user output files exist (status 60302).
Setting stageout wrapper exit info to {'exit_acronym': 'FAILED', 'exit_code': 60302, 'exit_msg': 'CmsCpFailure\ncmscp error message follows.\nERROR: Output file out_raw.root does not exist.'}.
====== Tue May  8 17:40:12 2018: Starting creation of user logs archive file.
Adding cmsRun-stdout.log to archive file cmsRun.log.tar.gz
Adding cmsRun-stderr.log to archive file cmsRun.log.tar.gz
Adding FrameworkJobReport.xml to archive file cmsRun.log.tar.gz
====== Tue May  8 17:40:12 2018: Finished creation of user logs archive file (status 0).
====== Tue May  8 17:40:12 2018: Starting initialization of stageout manager for local stageouts.
       -----> Stageout manager log start
StageOutMgr::__init__()
==== Stageout configuration start ====
Local Stage Out Implementation to be used is: gfal2
Local Stage Out PNN to be used is T3_US_TAMU
Local Stage Out Catalog to be used is trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/T3_US_TAMU/PhEDEx/storage.xml?protocol=direct
Trivial File Catalog has been loaded:
	lfn-to-pfn: protocol=direct path-match-re=/+store/cms/LoadTest/(.*) result=/fdata/hepx/store/cms/phedex_loadtest/$1
	lfn-to-pfn: protocol=direct path-match-re=/+store/(.*) result=/fdata/hepx/store/$1
	lfn-to-pfn: protocol=srm path-match-re=/+store/(.*) result=srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/$1 chain=direct
	lfn-to-pfn: protocol=srmv2 path-match-re=.*/LoadTest07_.*_TAMU_(.*)_.*_.* result=srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/PhEDEx_Debug/LoadTest07Source/T3_TAMU_$1
	lfn-to-pfn: protocol=srmv2 path-match-re=/+store/(.*) result=srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/$1 chain=direct
	lfn-to-pfn: protocol=xrootd path-match-re=/+store/(.*) result=root://xrootd.ba.infn.it//store/$1
	pfn-to-lfn: protocol=direct path-match-re=/+fdata/hepx/store/cms/phedex_loadtest/(.*) result=/store/LoadTest/$1
	pfn-to-lfn: protocol=direct path-match-re=/+fdata/hepx/store/cms/(.*) result=/store/$1
	pfn-to-lfn: protocol=srm path-match-re=.*\?SFN=/fdata/hepx/store/(.*) result=/store/$1 chain=direct
	pfn-to-lfn: protocol=srmv2 path-match-re=.*\?SFN=/fdata/hepx/store/(.*) result=/store/$1 chain=direct
There are 0 fallback stage out definitions.

==== Stageout configuration finish ====
       <----- Stageout manager log finish
Initialization was ok.
====== Tue May  8 17:40:12 2018: Finished initialization of stageout manager for local stageouts (status 0).
Will use gfal2 commands for direct stageout.
====== Tue May  8 17:40:12 2018: Starting initialization of stageout implementation for direct stageouts.
Initialization was ok.
====== Tue May  8 17:40:12 2018: Finished initialization of stageout implementation for direct stageouts (status 0).
====== Tue May  8 17:40:12 2018: Starting local stageout of user logs archive file.
       -----> Stageout manager log start
==>Working on file: /store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
===> Attempting Local Stage Out.
LFN to PFN match made:
LFN: /store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
PFN: /fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz

2018-05-08T17:40:12 : Creating output directory...
2018-05-08T17:40:12 : Running the stage out...
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
Tue May  8 17:40:12 UTC 2018
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
gfal-copy error: 30 (Read-only file system) - errno reported by local system call Read-only file system
Copying 1453 bytes file:///srv/cmsRun.log.tar.gz => file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
gfal-copy exit status: 30
Non-zero gfal-copy Exit status!!!
Cleaning up failed file:
Tue May  8 17:40:12 UTC 2018
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz	MISSING
2018-05-08T17:40:13 : Command exited with status: 23

ERROR: Exception During Stage Out:

2018-05-08T17:40:13 : Command exited non-zero
Attempt 1 to stage out failed.
Automatically retrying in 60 secs
 Error details:
StageOutError
Message: 2018-05-08T17:40:13 : Command exited non-zero
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	ErrorType : GeneralStageOutFailure
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : #!/bin/bash
env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-copy -t 2400 -T 2400 -p -K adler32  file:///srv/cmsRun.log.tar.gz file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz'
            EXIT_STATUS=$?
            echo "gfal-copy exit status: $EXIT_STATUS"
            if [[ $EXIT_STATUS != 0 ]]; then
               echo "Non-zero gfal-copy Exit status!!!"
               echo "Cleaning up failed file:"
                env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-rm -t 600 file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz '
               exit 60311
            fi
            exit 0
            
	LineNumber : 32
	ErrorNr : 0
	ExitCode : 23

Traceback: 



2018-05-08T17:41:13 : Running the stage out...
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
Tue May  8 17:41:13 UTC 2018
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
Copying 1453 bytes file:///srv/cmsRun.log.tar.gz => file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
gfal-copy exit status: 30
Non-zero gfal-copy Exit status!!!
Cleaning up failed file:
gfal-copy error: 30 (Read-only file system) - errno reported by local system call Read-only file system
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
Tue May  8 17:41:13 UTC 2018
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz	MISSING
2018-05-08T17:41:13 : Command exited with status: 23

ERROR: Exception During Stage Out:

2018-05-08T17:41:13 : Command exited non-zero
Attempt 2 to stage out failed.
Automatically retrying in 60 secs
 Error details:
StageOutError
Message: 2018-05-08T17:41:13 : Command exited non-zero
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	ErrorType : GeneralStageOutFailure
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : #!/bin/bash
env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-copy -t 2400 -T 2400 -p -K adler32  file:///srv/cmsRun.log.tar.gz file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz'
            EXIT_STATUS=$?
            echo "gfal-copy exit status: $EXIT_STATUS"
            if [[ $EXIT_STATUS != 0 ]]; then
               echo "Non-zero gfal-copy Exit status!!!"
               echo "Cleaning up failed file:"
                env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-rm -t 600 file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz '
               exit 60311
            fi
            exit 0
            
	LineNumber : 32
	ErrorNr : 0
	ExitCode : 23

Traceback: 
  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 213, in __call__
    self.executeCommand(command)

  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 91, in executeCommand
    raise StageOutError(msg, Command=command, ExitCode=exitCode)



===> Local Stage Out Failure for file:
======>  /store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz

===> Attempting 0 Fallback Stage Outs
Error during stageout: StageOutFailure
Message: Failure for local stage out:
StageOutError
Message: 2018-05-08T17:41:13 : Command exited non-zero
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	ErrorType : GeneralStageOutFailure
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : #!/bin/bash
env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-copy -t 2400 -T 2400 -p -K adler32  file:///srv/cmsRun.log.tar.gz file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz'
            EXIT_STATUS=$?
            echo "gfal-copy exit status: $EXIT_STATUS"
            if [[ $EXIT_STATUS != 0 ]]; then
               echo "Non-zero gfal-copy Exit status!!!"
               echo "Cleaning up failed file:"
                env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-rm -t 600 file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz '
               exit 60311
            fi
            exit 0
            
	LineNumber : 32
	ErrorNr : 0
	ExitCode : 23

Traceback: 
  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 213, in __call__
    self.executeCommand(command)

  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 91, in executeCommand
    raise StageOutError(msg, Command=command, ExitCode=exitCode)

Traceback (most recent call last):
  File "WMCore.zip/WMCore/Storage/StageOutMgr.py", line 314, in localStageOut
    impl(protocol, localPfn, pfn, options, checksums)
  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 224, in __call__
    raise ex
StageOutError: StageOutError
Message: 2018-05-08T17:41:13 : Command exited non-zero
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	ErrorType : GeneralStageOutFailure
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : #!/bin/bash
env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-copy -t 2400 -T 2400 -p -K adler32  file:///srv/cmsRun.log.tar.gz file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz'
            EXIT_STATUS=$?
            echo "gfal-copy exit status: $EXIT_STATUS"
            if [[ $EXIT_STATUS != 0 ]]; then
               echo "Non-zero gfal-copy Exit status!!!"
               echo "Cleaning up failed file:"
                env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-rm -t 600 file:///fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz '
               exit 60311
            fi
            exit 0
            
	LineNumber : 32
	ErrorNr : 0
	ExitCode : 23

Traceback: 
  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 213, in __call__
    self.executeCommand(command)

  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 91, in executeCommand
    raise StageOutError(msg, Command=command, ExitCode=exitCode)



	TargetPFN : /fdata/hepx/store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	LFN : /store/temp/user/dildick.4562d6ac929bbb5df19542e207f5d5ee11437abb/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : gfal2
	LineNumber : 32
	InputPFN : cmsRun.log.tar.gz
	Protocol : direct
	ErrorNr : 0
	ErrorType : GeneralStageOutFailure

Traceback: 
  File "WMCore.zip/WMCore/Storage/StageOutMgr.py", line 314, in localStageOut
    impl(protocol, localPfn, pfn, options, checksums)

  File "WMCore.zip/WMCore/Storage/StageOutImpl.py", line 224, in __call__
    raise ex


       <----- Stageout manager log finish
====== Tue May  8 17:41:13 2018: Finished local stageout of user logs archive file (status 60307).
====== Tue May  8 17:41:13 2018: Starting to clean local stageout area.
There are no files to remove in local temporary storage.
====== Tue May  8 17:41:13 2018: Finished to clean local stageout area.
====== Tue May  8 17:41:13 2018: Starting remote stageout of user logs archive file.
       -----> Stageout implementation log start
2018-05-08T17:41:13 : Creating output directory...
2018-05-08T17:41:13 : Running the stage out...
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
Tue May  8 17:41:13 UTC 2018
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
gfal-copy error: 17 (File exists) - Destination srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz exists and overwrite is not set
gfal-copy exit status: 17
Non-zero gfal-copy Exit status!!!
Cleaning up failed file:
Tue May  8 17:41:14 UTC 2018
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz	DELETED
2018-05-08T17:41:14 : Command exited with status: 23

ERROR: Exception During Stage Out:

2018-05-08T17:41:14 : Command exited non-zero
Attempt 1 to stage out failed.
Automatically retrying in 60 secs
 Error details:
StageOutError
Message: 2018-05-08T17:41:14 : Command exited non-zero
	ErrorCode : 60311
	ModuleName : WMCore.Storage.StageOutError
	MethodName : __init__
	ErrorType : GeneralStageOutFailure
	ClassInstance : None
	FileName : WMCore.zip/WMCore/Storage/StageOutError.py
	ClassName : None
	Command : #!/bin/bash
env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-copy -t 2400 -T 2400 -p   file:///srv/cmsRun.log.tar.gz srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz'
            EXIT_STATUS=$?
            echo "gfal-copy exit status: $EXIT_STATUS"
            if [[ $EXIT_STATUS != 0 ]]; then
               echo "Non-zero gfal-copy Exit status!!!"
               echo "Cleaning up failed file:"
                env -i X509_USER_PROXY=$X509_USER_PROXY JOBSTARTDIR=$JOBSTARTDIR bash -c '. $JOBSTARTDIR/startup_environment.sh; date; gfal-rm -t 600 srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz '
               exit 60311
            fi
            exit 0
            
	LineNumber : 32
	ErrorNr : 0
	ExitCode : 23

Traceback: 



2018-05-08T17:42:14 : Running the stage out...
/srv/startup_environment.sh: line 2: BASHOPTS: readonly variable
Tue May  8 17:42:14 UTC 2018
/srv/startup_environment.sh: line 9: BASH_VERSINFO: readonly variable
/srv/startup_environment.sh: line 30: EUID: readonly variable
/srv/startup_environment.sh: line 164: PPID: readonly variable
/srv/startup_environment.sh: line 176: SHELLOPTS: readonly variable
/srv/startup_environment.sh: line 215: UID: readonly variable
Copying 1453 bytes file:///srv/cmsRun.log.tar.gz => srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz
gfal-copy exit status: 0
2018-05-08T17:42:15 : Command exited with status: 0

       <----- Stageout implementation log finish
Setting storage_site = 'T3_US_TAMU', direct_stageout = True for file cmsRun.log.tar.gz in job report.
Dashboard stageout failure parameters: {'MonitorID': '180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT', 'MonitorJobID': '40_https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT_5', 'JobExitCode': 0, 'StageOutReport': [{'PSN': 'unknown', 'StageOutType': 'DIRECT', 'LFN': 'srm://srm.brazos.tamu.edu:8443/srm/v2/server?SFN=/fdata/hepx/store/user/dildick/DarkSUSY_mH_125_mGammaD_0250_13TeV/crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT/180508_133232/0000/failed/log/cmsRun_40.log.tar.gz', 'StageOutExit': 0, 'PNN': 'T3_US_TAMU', 'StageOutCommand': 'gfal2'}]}
====== Tue May  8 17:42:16 2018: Finished remote stageout of user logs archive file (status 0).
Job wrapper did not finish successfully (exit code 8028). Setting that same exit code for the stageout wrapper.
Stageout wrapper finished with exit code 8028. Will report failure to Dashboard.
Dashboard stageout failure parameters: {'MonitorID': '180508_133232:dildick_crab_DarkSUSY_mH_125_mGammaD_0250_13TeV_DIGI_HLT', 'MonitorJobID': '40_https://glidein.cern.ch/40/180508:133232:dildick:crab:DarkSUSY:mH:125:mGammaD:0250:13TeV:DIGI:HLT_5', 'JobExitCode': 8028}
====== Tue May  8 17:42:17 2018: cmscp.py FINISHING (status 8028).
======== Stageout at Tue May  8 17:42:17 GMT 2018 FINISHING (short status 92) ========
======== gWMS-CMSRunAnalysis.sh FINISHING at Tue May  8 17:42:17 GMT 2018 on c0112.brazos.tamu.edu with (short) status 92 ========
Local time: Tue May  8 17:42:17 UTC 2018
Short exit status: 92
======== Figuring out long exit code of the job for condor_chirp ========
==== Long exit code of the job is 8028 ====
======== Finished condor_chirp -ing the exit code of the job. Exit code of condor_chirp: 0 ========
Job Running time in seconds:  205
